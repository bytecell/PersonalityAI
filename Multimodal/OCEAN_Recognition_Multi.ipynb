{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "# from vgg16 import VGG16_LSTM\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import datetime\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, utils, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from facenet_pytorch import MTCNN\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from vivit import R2Plus1D_ViViT\n",
    "import gc\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "from ast_models_copy import ASTModel\n",
    "import math\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "frame_nums=15\n",
    "start_epoch = 0\n",
    "test_flage=False\n",
    "model_name =\"\"\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('[%(asctime)s]::%(module)s::%(levelname)s::%(message)s')\n",
    "streamHandler = logging.StreamHandler()\n",
    "streamHandler.setFormatter(formatter)\n",
    "fileHandler = logging.FileHandler('./LOG/personalityLog.log')\n",
    "fileHandler.setFormatter(formatter)\n",
    "logger.addHandler(streamHandler)\n",
    "logger.addHandler(fileHandler)\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-25 15:04:48,662]::1914516053::DEBUG::=============+Video Vision Transformer Strat!=============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "학습을 진행하는 기기: cuda\n"
     ]
    }
   ],
   "source": [
    "logger.debug('============={}+Video Vision Transformer Strat!============='.format(model_name))\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)\n",
    "\n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "print('학습을 진행하는 기기:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def getdata(pathname,datatype):\n",
    "    if datatype==\"train\":\n",
    "        final_data_set=[]\n",
    "        output_file=open(pathname, \"rb\")\n",
    "        for i in range(1):\n",
    "            final_data_set.extend(pickle.load(output_file))\n",
    "        return final_data_set\n",
    "    elif datatype==\"valid\":\n",
    "        final_data_set=[]\n",
    "        output_file=open(pathname, \"rb\")\n",
    "        for i in range(1):\n",
    "            final_data_set.extend(pickle.load(output_file))\n",
    "        return final_data_set\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "save_model_file_path = './save_model_folder/{}_{}.{}'\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add valid data path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# validation_set_data=getdata(\"/data/qx/secwayofKrData/15Frames/test_set.dat\",\"valid\")\n",
    "# print(1)\n",
    "validation_set_data=getdata(\"Your Path\",\"valid\")\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add train data path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# train_set_data=getdata(\"/data/qx/secwayofKrData/15Frames/train_set.dat\",\"train\")\n",
    "# print(1)\n",
    "train_set_data=getdata(\"Your Path\",\"train\")\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "# 데이터 텐서로 만들기\n",
    "\n",
    "class ChalearnDataset(Dataset):  \n",
    "    def __init__(self,audiodata,fullshot,textdata,tagdata, text_tokenizer, transform=None):\n",
    "        self.audiodata=audiodata\n",
    "        self.fullshot=fullshot\n",
    "        self.tagdata=tagdata\n",
    "        self.textdata=textdata\n",
    "        self.transform = transform  # 표준화 여부\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fullshot)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()#텐서의 경우 목록으로 돌아가기\n",
    "        audiodata = self.audiodata[idx]\n",
    "        audiodata = torch.FloatTensor(audiodata)\n",
    "        fullshot=self.fullshot[idx]\n",
    "        fullshot=torch.FloatTensor(fullshot)\n",
    "        textdata = text_tokenizer(self.textdata[idx],\n",
    "                                  return_tensors=\"pt\",\n",
    "                                     max_length=512,\n",
    "                                     padding=\"max_length\",\n",
    "                                     truncation=True)\n",
    "        textdata = {k: v.squeeze(0) for k, v in textdata.items()}  \n",
    "        \n",
    "        # image_data=image_data.reshape(15,3,224,224)\n",
    "        big_five_sorces=self.tagdata[idx]\n",
    "        big_five_sorces = torch.FloatTensor(big_five_sorces/120)\n",
    "\n",
    "        out = {\"audio\": audiodata,\n",
    "              \"image\": fullshot,\n",
    "              \"text_input_ids\": textdata[\"input_ids\"],\n",
    "              \"text_attn_mask\": textdata[\"attention_mask\"],\n",
    "              \"labels\": big_five_sorces}\n",
    "\n",
    "        \n",
    "        return out\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "num_workerssz = 20\n",
    "lr = 3e-05\n",
    "epochs = 120\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data 구성[[audio],[video],[transcript],[labels]]\n",
    "### 저희측에서 추출된 transcript는 하나의 시나리오 중에서 이 사람 말하는 모두 내용입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "text_tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')\n",
    "\n",
    "train_set_dataset = ChalearnDataset(audiodata= train_set_data[0], fullshot = train_set_data[1], textdata= train_set_data[2], \n",
    "                                 tagdata=train_set_data[3], transform=transform, text_tokenizer = text_tokenizer)\n",
    "val_set_dataset = ChalearnDataset(audiodata= validation_set_data[0], fullshot = validation_set_data[1], textdata= validation_set_data[2],\n",
    "                               tagdata=validation_set_data[3],transform=transform, text_tokenizer = text_tokenizer)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_set_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_set_dataset, batch_size=8, shuffle=True)\n",
    "max_value=0\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7917],\n",
       "        [0.7417],\n",
       "        [0.5667],\n",
       "        [0.5583],\n",
       "        [0.7250]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_dataset.__getitem__(10)['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads=12, hidden_size=768, d_out=0.1, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_attention_heads = num_heads\n",
    "        self.attention_head_size = int(hidden_size / num_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.dropout = nn.Dropout(d_out)\n",
    "\n",
    "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):        \n",
    "        batch_size = query.shape[0]\n",
    "        mixed_query_layer = self.query(query)\n",
    "\n",
    "        # [batch_size, n_heads, query_len, head_dim]\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(self.key(key))\n",
    "        value_layer = self.transpose_for_scores(self.value(value))\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        # [batch_size, n_heads, query_len, key_len]\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e10)\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # [batch_size, n_heads, query_len, head_dim]\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        # [batch_size, query_len, n_heads, head_dim]\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        # [batch_size, query_len, hid_dim]\n",
    "        context_layer = context_layer.view(batch_size, -1, self.hidden_size)\n",
    "\n",
    "        out = self.linear(context_layer)\n",
    "\n",
    "        return out, attention_probs\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, hidden_size=768, num_heads=12, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(num_heads=num_heads, hidden_size=hidden_size, d_out=dropout)\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size * 4, hidden_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "     #   query = self.norm1(query)\n",
    "        out, attn_probs = self.attn(query, key, value, mask)\n",
    "     #   out = query + self.dropout(out)  # Residual\n",
    "\n",
    "      #  out = self.norm2(out)\n",
    "      #  out = out + self.ffn(out)\n",
    "        \n",
    "        return out, attn_probs\n",
    "\n",
    "\n",
    "class MultiModalAttentionModel(nn.Module):\n",
    "    def __init__(self, text_model, image_model, audio_model, num_labels=5, projection_dim=768, d_out=0.1, ):\n",
    "        super().__init__()\n",
    "        self.projection_dim = projection_dim\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.text_model = text_model\n",
    "        self.image_model = image_model\n",
    "        self.audio_model = audio_model\n",
    "\n",
    "        # Freeze\n",
    "        for name, param in self.text_model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        for name, param in self.image_model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        for name, param in self.audio_model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        \n",
    "        self.mh_attention_1 = nn.ModuleList([AttentionBlock() for _ in range(6)])  # N-CA Blocks\n",
    "        self.mh_attention_2 = nn.ModuleList([AttentionBlock() for _ in range(6)])\n",
    "        self.mh_attention_3 = nn.ModuleList([AttentionBlock() for _ in range(6)])\n",
    "        self.mh_attention_4 = nn.ModuleList([AttentionBlock() for _ in range(6)])\n",
    "        self.mh_attention_5 = nn.ModuleList([AttentionBlock() for _ in range(6)])\n",
    "        self.mh_attention_6 = nn.ModuleList([AttentionBlock() for _ in range(6)])\n",
    "\n",
    "        # self.mh_attention_1 = MultiHeadAttention()\n",
    "        # self.mh_attention_2 = MultiHeadAttention()\n",
    "        # self.mh_attention_3 = MultiHeadAttention()\n",
    "\n",
    "        \n",
    "        self.dense = nn.Linear(self.projection_dim * 2, self.projection_dim * 2)\n",
    "        self.act = nn.GELU()\n",
    "        self.dropout = nn.Dropout(d_out)\n",
    "        self.out_proj = nn.Linear(self.projection_dim * 2, self.num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        audio,\n",
    "        image,\n",
    "        text_input_ids,\n",
    "        text_attn_mask,\n",
    "        labels = None,\n",
    "        position_ids = None,\n",
    "        head_mask = None,\n",
    "        inputs_embeds = None,\n",
    "        output_attentions = None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict = None,\n",
    "    ):\n",
    "\n",
    "        # attention 모듈 제외하고 모두 freeze\n",
    "        with torch.no_grad():\n",
    "            text_outputs = self.text_model(\n",
    "                input_ids = text_input_ids,\n",
    "                attention_mask = text_attn_mask,\n",
    "            )\n",
    "\n",
    "            image_outputs = self.image_model(image.permute(0,4,1,2,3).contiguous()).unsqueeze(1)\n",
    " \n",
    "            audio_outputs = self.audio_model(audio.squeeze()).unsqueeze(1)\n",
    "\n",
    "        text_embeds = text_outputs[0][:, 0, :].unsqueeze(1)\n",
    "        text_mask = text_attn_mask.unsqueeze(1).unsqueeze(2)  # [B, 1, 1, text_len]\n",
    "        # text_embeds = text_outputs[0][:,0,:]  # [batch_size, 768(hidden)]\n",
    "\n",
    "        # print(\"Text model out: \", text_embeds.shape)\n",
    "        # print(\"Image model out:\", image_outputs.shape)\n",
    "        # print(\"Audio model out:\", audio_outputs.shape)\n",
    "\n",
    "        # ca_a, ca_a_prob = self.mh_attention_1(audio_outputs, image_outputs, image_outputs)\n",
    "        # ca_at, ca_at_prob = self.mh_attention_2(audio_outputs, text_embeds, text_embeds)\n",
    "        # ca_t, ca_t_prob = self.mh_attention_1(text_embeds, image_outputs, image_outputs)\n",
    "        # ca_ta, ca_ta_prob = self.mh_attention_2(text_embeds, audio_outputs, audio_outputs)\n",
    "        # ca_v, ca_v_prob = self.mh_attention_1(image_outputs, text_embeds, text_embeds)\n",
    "        # ca_va, ca_va_prob = self.mh_attention_2(image_outputs, audio_outputs, audio_outputs)\n",
    "\n",
    "        image_outputs_cp = image_outputs.clone()\n",
    "        image_outputs_cp2 = image_outputs.clone()\n",
    "        text_outputs_cp = text_embeds.clone()\n",
    "        text_outputs_cp2 = text_embeds.clone()\n",
    "        audio_outputs_cp = audio_outputs.clone()\n",
    "\n",
    "        ## N CA Layers\n",
    "        #### Image Key\n",
    "        for idx, layer in enumerate(self.mh_attention_1):\n",
    "           # print(idx, \"th: \", image_outputs.shape)\n",
    "            # out, _ = layer(image_outputs, text_embeds, text_embeds)\n",
    "            # image_outputs = out\n",
    "            out, _ = layer(image_outputs, text_embeds, text_embeds)\n",
    "            image_outputs = out\n",
    "\n",
    "        for layer2 in self.mh_attention_2:\n",
    "            out2, _ = layer2(image_outputs_cp, audio_outputs, audio_outputs)\n",
    "            image_outputs_cp = out2\n",
    "\n",
    "        # #### Text key\n",
    "        # for layer3 in self.mh_attention_3:\n",
    "        #     out3, _ = layer3(text_embeds, audio_outputs, audio_outputs)\n",
    "        #     text_embeds = out3\n",
    "\n",
    "        # for layer4 in self.mh_attention_4:\n",
    "        #     out4, _ = layer4(text_outputs_cp, image_outputs_cp2, image_outputs_cp2)\n",
    "        #     text_outputs_cp = out4\n",
    "\n",
    "        # #### Audio key\n",
    "        # for layer5 in self.mh_attention_5:\n",
    "        #     out5, _ = layer5(audio_outputs, image_outputs_cp2, image_outputs_cp2)\n",
    "        #     audio_outputs = out5\n",
    "\n",
    "        # for layer6 in self.mh_attention_6:\n",
    "        #     out6, _ = layer6(audio_outputs_cp, text_outputs_cp2, text_outputs_cp2)\n",
    "        #     audio_outputs_cp = out6\n",
    "\n",
    "        # print(out.shape)\n",
    "        # print(out2.shape)\n",
    "\n",
    "\n",
    "\n",
    "        #################################  Don't forget  ######################\n",
    "        concated_input = torch.concat([out.squeeze(1), out2.squeeze(1),\n",
    "                                      # out3.squeeze(1), out4.squeeze(1),\n",
    "                                      # out5.squeeze(1), out6.squeeze(1)\n",
    "                                      ], dim=-1)  # [Batch_size, hidden * n_CA] \n",
    "        x = concated_input\n",
    "\n",
    "\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.act(x)  # GELU\n",
    "        x = self.dropout(x)\n",
    "        logits = self.out_proj(x)  # [Batch_Size x 5]\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if labels != None:\n",
    "            loss_fct = nn.L1Loss()\n",
    "    \n",
    "           # loss_fct = loss_fct(logits_O.view(-1, self.num_labels), labels[0])\n",
    "            loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/egg2018037024/Jupyter_Home/Multi-modal_personality/koRoBERTa_base-checkpoint16000 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------AST Model Summary---------------\n",
      "ImageNet pretraining: True, AudioSet pretraining: True\n",
      "audioset_pretrain == True\n",
      "[PE] Not use imagenet pretrain\n",
      "frequncey stride=10, time stride=10\n",
      "number of patches=599\n"
     ]
    }
   ],
   "source": [
    "#num_frames=frame_nums,batchsz = batchsz\n",
    "image_model=R2Plus1D_ViViT(2, # t\n",
    "                     14,  # h\n",
    "                     14,  # w\n",
    "                     2,   # patch_t\n",
    "                     2,   # patch_h\n",
    "                     2,   # patch_w\n",
    "                     5,   # num_classes\n",
    "                     768,  # dim\n",
    "                     6,   # depth\n",
    "                     10,  # heads\n",
    "                     8,   #  mlp_dim\n",
    "                     model=3) \n",
    "\n",
    "text_model = AutoModel.from_pretrained('/home/egg2018037024/Jupyter_Home/Multi-modal_personality/koRoBERTa_base-checkpoint16000')\n",
    "audio_model = ASTModel(input_tdim=1319, input_fdim=24, audioset_pretrain=True) # audio_split_samples, audio_length\n",
    "\n",
    "whole_model = MultiModalAttentionModel(text_model= text_model, \n",
    "                                       image_model = image_model, \n",
    "                                       audio_model = audio_model,\n",
    "                                      num_labels = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257531909"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in whole_model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "MultiModalAttentionModel                                          --\n",
       "├─RobertaModel: 1-1                                               --\n",
       "│    └─RobertaEmbeddings: 2-1                                     --\n",
       "│    │    └─Embedding: 3-1                                        (24,576,000)\n",
       "│    │    └─Embedding: 3-2                                        (394,752)\n",
       "│    │    └─Embedding: 3-3                                        (768)\n",
       "│    │    └─LayerNorm: 3-4                                        (1,536)\n",
       "│    │    └─Dropout: 3-5                                          --\n",
       "│    └─RobertaEncoder: 2-2                                        --\n",
       "│    │    └─ModuleList: 3-6                                       (85,054,464)\n",
       "│    └─RobertaPooler: 2-3                                         --\n",
       "│    │    └─Linear: 3-7                                           (590,592)\n",
       "│    │    └─Tanh: 3-8                                             --\n",
       "├─R2Plus1D_ViViT: 1-2                                             37,632\n",
       "│    └─VideoResNet: 2-4                                           --\n",
       "│    │    └─R2Plus1dStem: 3-9                                     (15,473)\n",
       "│    │    └─Sequential: 3-10                                      (444,032)\n",
       "│    │    └─Sequential: 3-11                                      (1,470,360)\n",
       "│    │    └─Sequential: 3-12                                      (5,874,736)\n",
       "│    │    └─Sequential: 3-13                                      (23,495,524)\n",
       "│    │    └─AdaptiveAvgPool3d: 3-14                               --\n",
       "│    │    └─Linear: 3-15                                          (205,200)\n",
       "│    └─PatchEmbed3D: 2-5                                          --\n",
       "│    │    └─Conv3d: 3-16                                          (25,166,592)\n",
       "│    │    └─LayerNorm: 3-17                                       (1,536)\n",
       "│    └─Sequential: 2-6                                            --\n",
       "│    │    └─Rearrange: 3-18                                       --\n",
       "│    │    └─Linear: 3-19                                          (3,146,496)\n",
       "│    └─Dropout: 2-7                                               --\n",
       "│    └─FSATransformerEncoder: 2-8                                 --\n",
       "│    │    └─ModuleList: 3-20                                      (1,221,168)\n",
       "│    └─Identity: 2-9                                              --\n",
       "│    └─Sequential: 2-10                                           --\n",
       "│    │    └─LayerNorm: 3-21                                       (1,536)\n",
       "│    │    └─Linear: 3-22                                          (3,845)\n",
       "├─ASTModel: 1-3                                                   --\n",
       "│    └─Conv1d: 2-11                                               (885,120)\n",
       "│    └─DistilledVisionTransformer: 2-12                           463,104\n",
       "│    │    └─PatchEmbed: 3-23                                      (197,376)\n",
       "│    │    └─Dropout: 3-24                                         --\n",
       "│    │    └─ModuleList: 3-25                                      (85,054,464)\n",
       "│    │    └─LayerNorm: 3-26                                       (1,536)\n",
       "│    │    └─Identity: 3-27                                        --\n",
       "│    │    └─Linear: 3-28                                          (769,000)\n",
       "│    │    └─Linear: 3-29                                          (769,000)\n",
       "│    └─Sequential: 2-13                                           --\n",
       "│    │    └─LayerNorm: 3-30                                       (1,536)\n",
       "│    │    └─Linear: 3-31                                          (590,592)\n",
       "├─ModuleList: 1-4                                                 --\n",
       "│    └─AttentionBlock: 2-14                                       --\n",
       "│    │    └─MultiHeadAttention: 3-32                              2,362,368\n",
       "│    │    └─LayerNorm: 3-33                                       1,536\n",
       "│    │    └─Dropout: 3-34                                         --\n",
       "│    │    └─Sequential: 3-35                                      4,722,432\n",
       "│    │    └─LayerNorm: 3-36                                       1,536\n",
       "│    └─AttentionBlock: 2-15                                       --\n",
       "│    │    └─MultiHeadAttention: 3-37                              2,362,368\n",
       "│    │    └─LayerNorm: 3-38                                       1,536\n",
       "│    │    └─Dropout: 3-39                                         --\n",
       "│    │    └─Sequential: 3-40                                      4,722,432\n",
       "│    │    └─LayerNorm: 3-41                                       1,536\n",
       "│    └─AttentionBlock: 2-16                                       --\n",
       "│    │    └─MultiHeadAttention: 3-42                              2,362,368\n",
       "│    │    └─LayerNorm: 3-43                                       1,536\n",
       "│    │    └─Dropout: 3-44                                         --\n",
       "│    │    └─Sequential: 3-45                                      4,722,432\n",
       "│    │    └─LayerNorm: 3-46                                       1,536\n",
       "│    └─AttentionBlock: 2-17                                       --\n",
       "│    │    └─MultiHeadAttention: 3-47                              2,362,368\n",
       "│    │    └─LayerNorm: 3-48                                       1,536\n",
       "│    │    └─Dropout: 3-49                                         --\n",
       "│    │    └─Sequential: 3-50                                      4,722,432\n",
       "│    │    └─LayerNorm: 3-51                                       1,536\n",
       "│    └─AttentionBlock: 2-18                                       --\n",
       "│    │    └─MultiHeadAttention: 3-52                              2,362,368\n",
       "│    │    └─LayerNorm: 3-53                                       1,536\n",
       "│    │    └─Dropout: 3-54                                         --\n",
       "│    │    └─Sequential: 3-55                                      4,722,432\n",
       "│    │    └─LayerNorm: 3-56                                       1,536\n",
       "│    └─AttentionBlock: 2-19                                       --\n",
       "│    │    └─MultiHeadAttention: 3-57                              2,362,368\n",
       "│    │    └─LayerNorm: 3-58                                       1,536\n",
       "│    │    └─Dropout: 3-59                                         --\n",
       "│    │    └─Sequential: 3-60                                      4,722,432\n",
       "│    │    └─LayerNorm: 3-61                                       1,536\n",
       "├─ModuleList: 1-5                                                 --\n",
       "│    └─AttentionBlock: 2-20                                       --\n",
       "│    │    └─MultiHeadAttention: 3-62                              2,362,368\n",
       "│    │    └─LayerNorm: 3-63                                       1,536\n",
       "│    │    └─Dropout: 3-64                                         --\n",
       "│    │    └─Sequential: 3-65                                      4,722,432\n",
       "│    │    └─LayerNorm: 3-66                                       1,536\n",
       "│    └─AttentionBlock: 2-21                                       --\n",
       "│    │    └─MultiHeadAttention: 3-67                              2,362,368\n",
       "│    │    └─LayerNorm: 3-68                                       1,536\n",
       "│    │    └─Dropout: 3-69                                         --\n",
       "│    │    └─Sequential: 3-70                                      4,722,432\n",
       "│    │    └─LayerNorm: 3-71                                       1,536\n",
       "│    └─AttentionBlock: 2-22                                       --\n",
       "│    │    └─MultiHeadAttention: 3-72                              2,362,368\n",
       "│    │    └─LayerNorm: 3-73                                       1,536\n",
       "│    │    └─Dropout: 3-74                                         --\n",
       "│    │    └─Sequential: 3-75                                      4,722,432\n",
       "│    │    └─LayerNorm: 3-76                                       1,536\n",
       "│    └─AttentionBlock: 2-23                                       --\n",
       "│    │    └─MultiHeadAttention: 3-77                              2,362,368\n",
       "│    │    └─LayerNorm: 3-78                                       1,536\n",
       "│    │    └─Dropout: 3-79                                         --\n",
       "│    │    └─Sequential: 3-80                                      4,722,432\n",
       "│    │    └─LayerNorm: 3-81                                       1,536\n",
       "│    └─AttentionBlock: 2-24                                       --\n",
       "│    │    └─MultiHeadAttention: 3-82                              2,362,368\n",
       "│    │    └─LayerNorm: 3-83                                       1,536\n",
       "│    │    └─Dropout: 3-84                                         --\n",
       "│    │    └─Sequential: 3-85                                      4,722,432\n",
       "│    │    └─LayerNorm: 3-86                                       1,536\n",
       "│    └─AttentionBlock: 2-25                                       --\n",
       "│    │    └─MultiHeadAttention: 3-87                              2,362,368\n",
       "│    │    └─LayerNorm: 3-88                                       1,536\n",
       "│    │    └─Dropout: 3-89                                         --\n",
       "│    │    └─Sequential: 3-90                                      4,722,432\n",
       "│    │    └─LayerNorm: 3-91                                       1,536\n",
       "├─ModuleList: 1-6                                                 --\n",
       "│    └─AttentionBlock: 2-26                                       --\n",
       "│    │    └─MultiHeadAttention: 3-92                              2,362,368\n",
       "│    │    └─LayerNorm: 3-93                                       1,536\n",
       "│    │    └─Dropout: 3-94                                         --\n",
       "│    │    └─Sequential: 3-95                                      4,722,432\n",
       "│    │    └─LayerNorm: 3-96                                       1,536\n",
       "│    └─AttentionBlock: 2-27                                       --\n",
       "│    │    └─MultiHeadAttention: 3-97                              2,362,368\n",
       "│    │    └─LayerNorm: 3-98                                       1,536\n",
       "│    │    └─Dropout: 3-99                                         --\n",
       "│    │    └─Sequential: 3-100                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-101                                      1,536\n",
       "│    └─AttentionBlock: 2-28                                       --\n",
       "│    │    └─MultiHeadAttention: 3-102                             2,362,368\n",
       "│    │    └─LayerNorm: 3-103                                      1,536\n",
       "│    │    └─Dropout: 3-104                                        --\n",
       "│    │    └─Sequential: 3-105                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-106                                      1,536\n",
       "│    └─AttentionBlock: 2-29                                       --\n",
       "│    │    └─MultiHeadAttention: 3-107                             2,362,368\n",
       "│    │    └─LayerNorm: 3-108                                      1,536\n",
       "│    │    └─Dropout: 3-109                                        --\n",
       "│    │    └─Sequential: 3-110                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-111                                      1,536\n",
       "│    └─AttentionBlock: 2-30                                       --\n",
       "│    │    └─MultiHeadAttention: 3-112                             2,362,368\n",
       "│    │    └─LayerNorm: 3-113                                      1,536\n",
       "│    │    └─Dropout: 3-114                                        --\n",
       "│    │    └─Sequential: 3-115                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-116                                      1,536\n",
       "│    └─AttentionBlock: 2-31                                       --\n",
       "│    │    └─MultiHeadAttention: 3-117                             2,362,368\n",
       "│    │    └─LayerNorm: 3-118                                      1,536\n",
       "│    │    └─Dropout: 3-119                                        --\n",
       "│    │    └─Sequential: 3-120                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-121                                      1,536\n",
       "├─ModuleList: 1-7                                                 --\n",
       "│    └─AttentionBlock: 2-32                                       --\n",
       "│    │    └─MultiHeadAttention: 3-122                             2,362,368\n",
       "│    │    └─LayerNorm: 3-123                                      1,536\n",
       "│    │    └─Dropout: 3-124                                        --\n",
       "│    │    └─Sequential: 3-125                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-126                                      1,536\n",
       "│    └─AttentionBlock: 2-33                                       --\n",
       "│    │    └─MultiHeadAttention: 3-127                             2,362,368\n",
       "│    │    └─LayerNorm: 3-128                                      1,536\n",
       "│    │    └─Dropout: 3-129                                        --\n",
       "│    │    └─Sequential: 3-130                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-131                                      1,536\n",
       "│    └─AttentionBlock: 2-34                                       --\n",
       "│    │    └─MultiHeadAttention: 3-132                             2,362,368\n",
       "│    │    └─LayerNorm: 3-133                                      1,536\n",
       "│    │    └─Dropout: 3-134                                        --\n",
       "│    │    └─Sequential: 3-135                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-136                                      1,536\n",
       "│    └─AttentionBlock: 2-35                                       --\n",
       "│    │    └─MultiHeadAttention: 3-137                             2,362,368\n",
       "│    │    └─LayerNorm: 3-138                                      1,536\n",
       "│    │    └─Dropout: 3-139                                        --\n",
       "│    │    └─Sequential: 3-140                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-141                                      1,536\n",
       "│    └─AttentionBlock: 2-36                                       --\n",
       "│    │    └─MultiHeadAttention: 3-142                             2,362,368\n",
       "│    │    └─LayerNorm: 3-143                                      1,536\n",
       "│    │    └─Dropout: 3-144                                        --\n",
       "│    │    └─Sequential: 3-145                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-146                                      1,536\n",
       "│    └─AttentionBlock: 2-37                                       --\n",
       "│    │    └─MultiHeadAttention: 3-147                             2,362,368\n",
       "│    │    └─LayerNorm: 3-148                                      1,536\n",
       "│    │    └─Dropout: 3-149                                        --\n",
       "│    │    └─Sequential: 3-150                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-151                                      1,536\n",
       "├─ModuleList: 1-8                                                 --\n",
       "│    └─AttentionBlock: 2-38                                       --\n",
       "│    │    └─MultiHeadAttention: 3-152                             2,362,368\n",
       "│    │    └─LayerNorm: 3-153                                      1,536\n",
       "│    │    └─Dropout: 3-154                                        --\n",
       "│    │    └─Sequential: 3-155                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-156                                      1,536\n",
       "│    └─AttentionBlock: 2-39                                       --\n",
       "│    │    └─MultiHeadAttention: 3-157                             2,362,368\n",
       "│    │    └─LayerNorm: 3-158                                      1,536\n",
       "│    │    └─Dropout: 3-159                                        --\n",
       "│    │    └─Sequential: 3-160                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-161                                      1,536\n",
       "│    └─AttentionBlock: 2-40                                       --\n",
       "│    │    └─MultiHeadAttention: 3-162                             2,362,368\n",
       "│    │    └─LayerNorm: 3-163                                      1,536\n",
       "│    │    └─Dropout: 3-164                                        --\n",
       "│    │    └─Sequential: 3-165                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-166                                      1,536\n",
       "│    └─AttentionBlock: 2-41                                       --\n",
       "│    │    └─MultiHeadAttention: 3-167                             2,362,368\n",
       "│    │    └─LayerNorm: 3-168                                      1,536\n",
       "│    │    └─Dropout: 3-169                                        --\n",
       "│    │    └─Sequential: 3-170                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-171                                      1,536\n",
       "│    └─AttentionBlock: 2-42                                       --\n",
       "│    │    └─MultiHeadAttention: 3-172                             2,362,368\n",
       "│    │    └─LayerNorm: 3-173                                      1,536\n",
       "│    │    └─Dropout: 3-174                                        --\n",
       "│    │    └─Sequential: 3-175                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-176                                      1,536\n",
       "│    └─AttentionBlock: 2-43                                       --\n",
       "│    │    └─MultiHeadAttention: 3-177                             2,362,368\n",
       "│    │    └─LayerNorm: 3-178                                      1,536\n",
       "│    │    └─Dropout: 3-179                                        --\n",
       "│    │    └─Sequential: 3-180                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-181                                      1,536\n",
       "├─ModuleList: 1-9                                                 --\n",
       "│    └─AttentionBlock: 2-44                                       --\n",
       "│    │    └─MultiHeadAttention: 3-182                             2,362,368\n",
       "│    │    └─LayerNorm: 3-183                                      1,536\n",
       "│    │    └─Dropout: 3-184                                        --\n",
       "│    │    └─Sequential: 3-185                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-186                                      1,536\n",
       "│    └─AttentionBlock: 2-45                                       --\n",
       "│    │    └─MultiHeadAttention: 3-187                             2,362,368\n",
       "│    │    └─LayerNorm: 3-188                                      1,536\n",
       "│    │    └─Dropout: 3-189                                        --\n",
       "│    │    └─Sequential: 3-190                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-191                                      1,536\n",
       "│    └─AttentionBlock: 2-46                                       --\n",
       "│    │    └─MultiHeadAttention: 3-192                             2,362,368\n",
       "│    │    └─LayerNorm: 3-193                                      1,536\n",
       "│    │    └─Dropout: 3-194                                        --\n",
       "│    │    └─Sequential: 3-195                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-196                                      1,536\n",
       "│    └─AttentionBlock: 2-47                                       --\n",
       "│    │    └─MultiHeadAttention: 3-197                             2,362,368\n",
       "│    │    └─LayerNorm: 3-198                                      1,536\n",
       "│    │    └─Dropout: 3-199                                        --\n",
       "│    │    └─Sequential: 3-200                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-201                                      1,536\n",
       "│    └─AttentionBlock: 2-48                                       --\n",
       "│    │    └─MultiHeadAttention: 3-202                             2,362,368\n",
       "│    │    └─LayerNorm: 3-203                                      1,536\n",
       "│    │    └─Dropout: 3-204                                        --\n",
       "│    │    └─Sequential: 3-205                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-206                                      1,536\n",
       "│    └─AttentionBlock: 2-49                                       --\n",
       "│    │    └─MultiHeadAttention: 3-207                             2,362,368\n",
       "│    │    └─LayerNorm: 3-208                                      1,536\n",
       "│    │    └─Dropout: 3-209                                        --\n",
       "│    │    └─Sequential: 3-210                                     4,722,432\n",
       "│    │    └─LayerNorm: 3-211                                      1,536\n",
       "├─Linear: 1-10                                                    2,360,832\n",
       "├─GELU: 1-11                                                      --\n",
       "├─Dropout: 1-12                                                   --\n",
       "├─Linear: 1-13                                                    7,685\n",
       "==========================================================================================\n",
       "Total params: 517,965,879\n",
       "Trainable params: 257,531,909\n",
       "Non-trainable params: 260,433,970\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(whole_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from transformers import EvalPrediction\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import torch\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "    \n",
    "    MAE = 1 - mean_absolute_error(labels.squeeze(), preds)\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        '1 - MAE' : MAE\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, TrainerCallback, DefaultDataCollator\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/home/user/10TB/personalityAI/multimodal/6CA_Ikey\",\n",
    "    logging_dir= \"/home/user/10TB/personalityAI/multimodal/6CA_Ikey_log\",\n",
    "    num_train_epochs=10,\n",
    "    learning_rate = 3e-5, \n",
    "   # max_steps=1000,\n",
    "    per_device_train_batch_size=32,\n",
    "#    gradient_accumulation_steps = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "#    eval_accumulation_steps = 32,\n",
    "    logging_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    lr_scheduler_type = \"linear\",\n",
    " #   dataloader_num_workers = 12,\n",
    "    warmup_ratio = 0.1,\n",
    "#    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=whole_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set_dataset,\n",
    "    eval_dataset=val_set_dataset,\n",
    "   # data_collator=DefaultDataCollator(return_tensors = \"pt\"),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()   #0.912889"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# train_set_data=getdata(\"/data/qx/secwayofKrData/15Frames/train_set.dat\",\"train\")\n",
    "# print(1)\n",
    "test_set_data=getdata(\"Your Path\",\"valid\")\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_dataset = ChalearnDataset(audiodata= test_set_data[0], fullshot = test_set_data[1], textdata= test_set_data[2],\n",
    "                               tagdata=test_set_data[3],transform=transform, text_tokenizer = text_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    " def test_compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "    \n",
    "    labels = labels.squeeze().T\n",
    "    preds = preds.T\n",
    "    \n",
    "    MAE = []\n",
    "    \n",
    "    for i in range(5):\n",
    "        MAE.append(1 - mean_absolute_error(labels[i], preds[i]))\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'Avg' : np.mean(MAE),\n",
    "        '1 - MAE' : MAE\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set(), [])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from safetensors.torch import load_model\n",
    "\n",
    "test_model = MultiModalAttentionModel(text_model= text_model, \n",
    "                                       image_model = image_model, \n",
    "                                       audio_model = audio_model,\n",
    "                                      num_labels = 5)\n",
    "\n",
    "\n",
    "load_model(test_model, \"/home/user/10TB/personalityAI/multimodal/6CA_Ikey/checkpoint-238/model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, TrainerCallback, DefaultDataCollator\n",
    "\n",
    "test_training_args = TrainingArguments(\n",
    "    output_dir=\"/home/user/10TB/personalityAI/multimodal/6CA_Ikey\",\n",
    "    logging_dir= \"/home/user/10TB/personalityAI/multimodal/6CA_Ikey_log\",\n",
    "    num_train_epochs=10,\n",
    "    learning_rate = 3e-5,  \n",
    "   # max_steps=1000,\n",
    "    per_device_train_batch_size=32,\n",
    "#    gradient_accumulation_steps = 16,\n",
    "    per_device_eval_batch_size = 32,\n",
    "#    eval_accumulation_steps = 32,\n",
    "    logging_strategy = \"epoch\",\n",
    "    lr_scheduler_type = \"linear\",\n",
    " #   dataloader_num_workers = 12,\n",
    "    warmup_ratio = 0.1,\n",
    "#    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "eval_trainer = Trainer(\n",
    "    model=test_model,\n",
    "    args=test_training_args,\n",
    "    data_collator=DefaultDataCollator(return_tensors = \"pt\"),\n",
    "    compute_metrics=test_compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/egg2018037024/venv310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 11:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.09143953770399094,\n",
       " 'eval_Avg': 0.9085604593157768,\n",
       " 'eval_1 - MAE': [0.9117942899465561,\n",
       "  0.9110761284828186,\n",
       "  0.8928183913230896,\n",
       "  0.9298798218369484,\n",
       "  0.8972336649894714],\n",
       " 'eval_runtime': 716.0798,\n",
       " 'eval_samples_per_second': 7.105,\n",
       " 'eval_steps_per_second': 0.056}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_trainer.evaluate(test_set_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
