{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "# from vgg16 import VGG16_LSTM\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import datetime\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, utils, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from facenet_pytorch import MTCNN\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from vivit import R2Plus1D_ViViT\n",
    "import gc\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "batchsz = 4\n",
    "frame_nums=15\n",
    "start_epoch = 0\n",
    "test_flage=False\n",
    "model_name =\"\"\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('[%(asctime)s]::%(module)s::%(levelname)s::%(message)s')\n",
    "streamHandler = logging.StreamHandler()\n",
    "streamHandler.setFormatter(formatter)\n",
    "fileHandler = logging.FileHandler('./LOG/personalityLog.log')\n",
    "fileHandler.setFormatter(formatter)\n",
    "logger.addHandler(streamHandler)\n",
    "logger.addHandler(fileHandler)\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-07 01:28:19,966]::3184030073::DEBUG::=============/ssd_data/qx/KETIdata_by_scenario/full_video/dataset/divided_by_person/15frame/scene1+Video Vision Transformer Strat!=============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "학습을 진행하는 기기: cuda:0\n"
     ]
    }
   ],
   "source": [
    "logger.debug('============={}+Video Vision Transformer Strat!============='.format(model_name))\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)\n",
    "\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "print('학습을 진행하는 기기:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def getdata(pathname,datatype):\n",
    "    if datatype==\"train\":\n",
    "        final_data_set=[]\n",
    "        output_file=open(pathname, \"rb\")\n",
    "        for i in range(1):\n",
    "            final_data_set.extend(pickle.load(output_file))\n",
    "        return final_data_set\n",
    "    elif datatype==\"valid\":\n",
    "        final_data_set=[]\n",
    "        output_file=open(pathname, \"rb\")\n",
    "        for i in range(1):\n",
    "            final_data_set.extend(pickle.load(output_file))\n",
    "        return final_data_set\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_model_file_path = './save_model_folder/{}_{}.{}'\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add valid data path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# validation_set_data=getdata(\"/data/qx/secwayofKrData/15Frames/test_set.dat\",\"valid\")\n",
    "# print(1)\n",
    "validation_set_data=getdata(\"Your Path\",\"valid\")\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add train data path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# train_set_data=getdata(\"/data/qx/secwayofKrData/15Frames/train_set.dat\",\"train\")\n",
    "# print(1)\n",
    "train_set_data=getdata(\"Your Path\",\"train\")\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "class ChalearnDataset(Dataset):\n",
    "    def __init__(self,fullshot,tagdata,transform=None):\n",
    "        self.fullshot=fullshot\n",
    "        self.tagdata=tagdata\n",
    "        self.transform = transform  # 표준화 여부\n",
    "    def __len__(self):\n",
    "        return len(self.fullshot)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()#텐서의 경우 목록으로 돌아가기\n",
    "        fullshot=self.fullshot[idx]\n",
    "        fullshot=torch.FloatTensor(fullshot)\n",
    "        big_five_sorces=self.tagdata[idx]\n",
    "        big_five_sorces = torch.FloatTensor(big_five_sorces/120)\n",
    "        return fullshot,big_five_sorces\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "num_workerssz = 20\n",
    "lr = 3e-05\n",
    "epochs = 120\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data 구성[[audio],[video],[transcript],[labels]]\n",
    "### 저희측에서 추출된 transcript는 하나의 시나리오 중에서 이 사람 말하는 모두 내용입니다다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "train_set_data = ChalearnDataset(fullshot = train_set_data[1],tagdata=train_set_data[3],transform=transform)\n",
    "val_set_data = ChalearnDataset(fullshot = validation_set_data[1],tagdata=validation_set_data[3],transform=transform)\n",
    "train_dataloader = DataLoader(dataset=train_set_data, batch_size=batchsz, shuffle=True, num_workers=num_workerssz)\n",
    "val_dataloader = DataLoader(dataset=val_set_data, batch_size=batchsz, shuffle=True, num_workers=num_workerssz)\n",
    "max_value=0\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#num_frames=frame_nums,batchsz = batchsz\n",
    "model=R2Plus1D_ViViT(2, # t\n",
    "                     14,  # h\n",
    "                     14,  # w\n",
    "                     2,   # patch_t\n",
    "                     2,   # patch_h\n",
    "                     2,   # patch_w\n",
    "                     5,   # num_classes\n",
    "                     96,  # dim\n",
    "                     6,   # depth\n",
    "                     10,  # heads\n",
    "                     8,   #  mlp_dim\n",
    "                     model=3) \n",
    "\n",
    "model=model.to(device)\n",
    "criterion = torch.nn.L1Loss().to(device) # 손실함수\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr) # 옵티마이저\n",
    "\n",
    "if test_flage==True:\n",
    "    checkpoint=torch.load(save_model_file_path.format('model',start_epoch,'pth'), map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    criterion.load_state_dict(checkpoint[\"loss\"])\n",
    "train_avg_loss0=[]\n",
    "val_avg_loss0=[]\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================================================\n",
      "Layer (type:depth-idx)                             Input Shape               Output Shape              Param #\n",
      "=============================================================================================================================\n",
      "R2Plus1D_ViViT                                     [4, 3, 15, 224, 224]      [4, 5]                    3,146,016\n",
      "├─VideoResNet: 1-1                                 [4, 3, 15, 224, 224]      [4, 512, 2, 14, 14]       205,200\n",
      "│    └─R2Plus1dStem: 2-1                           [4, 3, 15, 224, 224]      [4, 64, 15, 112, 112]     --\n",
      "│    │    └─Conv3d: 3-1                            [4, 3, 15, 224, 224]      [4, 45, 15, 112, 112]     6,615\n",
      "│    │    └─BatchNorm3d: 3-2                       [4, 45, 15, 112, 112]     [4, 45, 15, 112, 112]     90\n",
      "│    │    └─ReLU: 3-3                              [4, 45, 15, 112, 112]     [4, 45, 15, 112, 112]     --\n",
      "│    │    └─Conv3d: 3-4                            [4, 45, 15, 112, 112]     [4, 64, 15, 112, 112]     8,640\n",
      "│    │    └─BatchNorm3d: 3-5                       [4, 64, 15, 112, 112]     [4, 64, 15, 112, 112]     128\n",
      "│    │    └─ReLU: 3-6                              [4, 64, 15, 112, 112]     [4, 64, 15, 112, 112]     --\n",
      "│    └─Sequential: 2-2                             [4, 64, 15, 112, 112]     [4, 64, 15, 112, 112]     --\n",
      "│    │    └─BasicBlock: 3-7                        [4, 64, 15, 112, 112]     [4, 64, 15, 112, 112]     222,016\n",
      "│    │    └─BasicBlock: 3-8                        [4, 64, 15, 112, 112]     [4, 64, 15, 112, 112]     222,016\n",
      "│    └─Sequential: 2-3                             [4, 64, 15, 112, 112]     [4, 128, 8, 56, 56]       --\n",
      "│    │    └─BasicBlock: 3-9                        [4, 64, 15, 112, 112]     [4, 128, 8, 56, 56]       583,960\n",
      "│    │    └─BasicBlock: 3-10                       [4, 128, 8, 56, 56]       [4, 128, 8, 56, 56]       886,400\n",
      "│    └─Sequential: 2-4                             [4, 128, 8, 56, 56]       [4, 256, 4, 28, 28]       --\n",
      "│    │    └─BasicBlock: 3-11                       [4, 128, 8, 56, 56]       [4, 256, 4, 28, 28]       2,332,464\n",
      "│    │    └─BasicBlock: 3-12                       [4, 256, 4, 28, 28]       [4, 256, 4, 28, 28]       3,542,272\n",
      "│    └─Sequential: 2-5                             [4, 256, 4, 28, 28]       [4, 512, 2, 14, 14]       --\n",
      "│    │    └─BasicBlock: 3-13                       [4, 256, 4, 28, 28]       [4, 512, 2, 14, 14]       9,333,092\n",
      "│    │    └─BasicBlock: 3-14                       [4, 512, 2, 14, 14]       [4, 512, 2, 14, 14]       14,162,432\n",
      "├─Sequential: 1-2                                  [4, 512, 2, 14, 14]       [4, 1, 49, 96]            --\n",
      "│    └─Rearrange: 2-6                              [4, 512, 2, 14, 14]       [4, 1, 49, 4096]          --\n",
      "│    └─Linear: 2-7                                 [4, 1, 49, 4096]          [4, 1, 49, 96]            393,312\n",
      "├─Dropout: 1-3                                     [4, 1, 49, 96]            [4, 1, 49, 96]            --\n",
      "├─FSATransformerEncoder: 1-4                       [4, 1, 49, 96]            [4, 49, 96]               --\n",
      "│    └─ModuleList: 2-8                             --                        --                        --\n",
      "│    │    └─ModuleList: 3-15                       --                        --                        25,448\n",
      "│    │    └─ModuleList: 3-16                       --                        --                        25,448\n",
      "│    │    └─ModuleList: 3-17                       --                        --                        25,448\n",
      "│    │    └─ModuleList: 3-18                       --                        --                        25,448\n",
      "│    │    └─ModuleList: 3-19                       --                        --                        25,448\n",
      "│    │    └─ModuleList: 3-20                       --                        --                        25,448\n",
      "├─Identity: 1-5                                    [4, 96]                   [4, 96]                   --\n",
      "├─Sequential: 1-6                                  [4, 96]                   [4, 5]                    --\n",
      "│    └─LayerNorm: 2-9                              [4, 96]                   [4, 96]                   192\n",
      "│    └─Linear: 2-10                                [4, 96]                   [4, 5]                    485\n",
      "=============================================================================================================================\n",
      "Total params: 35,198,018\n",
      "Trainable params: 35,198,018\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 623.70\n",
      "=============================================================================================================================\n",
      "Input size (MB): 36.13\n",
      "Forward/backward pass size (MB): 15309.69\n",
      "Params size (MB): 127.39\n",
      "Estimated Total Size (MB): 15473.21\n",
      "=============================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type:depth-idx)                             Input Shape               Output Shape              Param #\n",
       "=============================================================================================================================\n",
       "R2Plus1D_ViViT                                     [4, 3, 15, 224, 224]      [4, 5]                    3,146,016\n",
       "├─VideoResNet: 1-1                                 [4, 3, 15, 224, 224]      [4, 512, 2, 14, 14]       205,200\n",
       "│    └─R2Plus1dStem: 2-1                           [4, 3, 15, 224, 224]      [4, 64, 15, 112, 112]     --\n",
       "│    │    └─Conv3d: 3-1                            [4, 3, 15, 224, 224]      [4, 45, 15, 112, 112]     6,615\n",
       "│    │    └─BatchNorm3d: 3-2                       [4, 45, 15, 112, 112]     [4, 45, 15, 112, 112]     90\n",
       "│    │    └─ReLU: 3-3                              [4, 45, 15, 112, 112]     [4, 45, 15, 112, 112]     --\n",
       "│    │    └─Conv3d: 3-4                            [4, 45, 15, 112, 112]     [4, 64, 15, 112, 112]     8,640\n",
       "│    │    └─BatchNorm3d: 3-5                       [4, 64, 15, 112, 112]     [4, 64, 15, 112, 112]     128\n",
       "│    │    └─ReLU: 3-6                              [4, 64, 15, 112, 112]     [4, 64, 15, 112, 112]     --\n",
       "│    └─Sequential: 2-2                             [4, 64, 15, 112, 112]     [4, 64, 15, 112, 112]     --\n",
       "│    │    └─BasicBlock: 3-7                        [4, 64, 15, 112, 112]     [4, 64, 15, 112, 112]     222,016\n",
       "│    │    └─BasicBlock: 3-8                        [4, 64, 15, 112, 112]     [4, 64, 15, 112, 112]     222,016\n",
       "│    └─Sequential: 2-3                             [4, 64, 15, 112, 112]     [4, 128, 8, 56, 56]       --\n",
       "│    │    └─BasicBlock: 3-9                        [4, 64, 15, 112, 112]     [4, 128, 8, 56, 56]       583,960\n",
       "│    │    └─BasicBlock: 3-10                       [4, 128, 8, 56, 56]       [4, 128, 8, 56, 56]       886,400\n",
       "│    └─Sequential: 2-4                             [4, 128, 8, 56, 56]       [4, 256, 4, 28, 28]       --\n",
       "│    │    └─BasicBlock: 3-11                       [4, 128, 8, 56, 56]       [4, 256, 4, 28, 28]       2,332,464\n",
       "│    │    └─BasicBlock: 3-12                       [4, 256, 4, 28, 28]       [4, 256, 4, 28, 28]       3,542,272\n",
       "│    └─Sequential: 2-5                             [4, 256, 4, 28, 28]       [4, 512, 2, 14, 14]       --\n",
       "│    │    └─BasicBlock: 3-13                       [4, 256, 4, 28, 28]       [4, 512, 2, 14, 14]       9,333,092\n",
       "│    │    └─BasicBlock: 3-14                       [4, 512, 2, 14, 14]       [4, 512, 2, 14, 14]       14,162,432\n",
       "├─Sequential: 1-2                                  [4, 512, 2, 14, 14]       [4, 1, 49, 96]            --\n",
       "│    └─Rearrange: 2-6                              [4, 512, 2, 14, 14]       [4, 1, 49, 4096]          --\n",
       "│    └─Linear: 2-7                                 [4, 1, 49, 4096]          [4, 1, 49, 96]            393,312\n",
       "├─Dropout: 1-3                                     [4, 1, 49, 96]            [4, 1, 49, 96]            --\n",
       "├─FSATransformerEncoder: 1-4                       [4, 1, 49, 96]            [4, 49, 96]               --\n",
       "│    └─ModuleList: 2-8                             --                        --                        --\n",
       "│    │    └─ModuleList: 3-15                       --                        --                        25,448\n",
       "│    │    └─ModuleList: 3-16                       --                        --                        25,448\n",
       "│    │    └─ModuleList: 3-17                       --                        --                        25,448\n",
       "│    │    └─ModuleList: 3-18                       --                        --                        25,448\n",
       "│    │    └─ModuleList: 3-19                       --                        --                        25,448\n",
       "│    │    └─ModuleList: 3-20                       --                        --                        25,448\n",
       "├─Identity: 1-5                                    [4, 96]                   [4, 96]                   --\n",
       "├─Sequential: 1-6                                  [4, 96]                   [4, 5]                    --\n",
       "│    └─LayerNorm: 2-9                              [4, 96]                   [4, 96]                   192\n",
       "│    └─Linear: 2-10                                [4, 96]                   [4, 5]                    485\n",
       "=============================================================================================================================\n",
       "Total params: 35,198,018\n",
       "Trainable params: 35,198,018\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 623.70\n",
       "=============================================================================================================================\n",
       "Input size (MB): 36.13\n",
       "Forward/backward pass size (MB): 15309.69\n",
       "Params size (MB): 127.39\n",
       "Estimated Total Size (MB): 15473.21\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size = ((4, 3,15, 224, 224)), col_names = ['input_size','output_size','num_params'],verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ocean(data,batch_size):\n",
    "    o=[]\n",
    "    c=[]\n",
    "    e=[]\n",
    "    a=[]\n",
    "    n=[]\n",
    "    if batch_size >1:\n",
    "        for i in range(batch_size):\n",
    "            o.append(data[i][0].item())\n",
    "            c.append(data[i][1].item())\n",
    "            e.append(data[i][2].item())\n",
    "            a.append(data[i][3].item())\n",
    "            n.append(data[i][4].item())\n",
    "        o = torch.tensor(o) \n",
    "        c = torch.tensor(c) \n",
    "        e = torch.tensor(e) \n",
    "        a = torch.tensor(a) \n",
    "        n = torch.tensor(n) \n",
    "    elif batch_size == 1:\n",
    "        data=data.squeeze()\n",
    "        for i in range(batch_size):\n",
    "            o.append(data[0].item())\n",
    "            c.append(data[1].item())\n",
    "            e.append(data[2].item())\n",
    "            a.append(data[3].item())\n",
    "            n.append(data[4].item())\n",
    "            \n",
    "        o = torch.tensor(o) \n",
    "        c = torch.tensor(c) \n",
    "        e = torch.tensor(e) \n",
    "        a = torch.tensor(a) \n",
    "        n = torch.tensor(n)\n",
    "    return o,c,e,a,n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2262/2262 [06:26<00:00,  5.85it/s]\n",
      "100%|█████████▉| 829/831 [01:02<00:00, 15.25it/s]/home/user/anaconda3/envs/qx/lib/python3.10/site-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([1, 5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "100%|██████████| 831/831 [01:03<00:00, 13.07it/s]\n",
      "[2025-05-07 01:36:07,798]::1259713826::DEBUG::Epoch: 1 , 1-MAE: 0.9068 , 1-Training Loss: 0.9411 , 1-val_avg_o_loss:0.8998 , 1-val_avg_c_loss:0.9050 , 1-val_avg_e_loss:0.8937 , 1-val_avg_a_loss:0.9339 , 1-val_avg_n_loss:0.9016\n",
      "100%|██████████| 2262/2262 [06:49<00:00,  5.53it/s]\n",
      "100%|██████████| 831/831 [01:04<00:00, 12.88it/s]\n",
      "[2025-05-07 01:44:02,941]::1259713826::DEBUG::Epoch: 2 , 1-MAE: 0.9069 , 1-Training Loss: 0.9700 , 1-val_avg_o_loss:0.8977 , 1-val_avg_c_loss:0.9060 , 1-val_avg_e_loss:0.8941 , 1-val_avg_a_loss:0.9332 , 1-val_avg_n_loss:0.9034\n",
      "100%|██████████| 2262/2262 [06:51<00:00,  5.50it/s]\n",
      "100%|██████████| 831/831 [01:04<00:00, 12.93it/s]\n",
      "[2025-05-07 01:52:00,423]::1259713826::DEBUG::Epoch: 3 , 1-MAE: 0.9076 , 1-Training Loss: 0.9771 , 1-val_avg_o_loss:0.8990 , 1-val_avg_c_loss:0.9071 , 1-val_avg_e_loss:0.8942 , 1-val_avg_a_loss:0.9350 , 1-val_avg_n_loss:0.9029\n",
      "100%|██████████| 2262/2262 [06:50<00:00,  5.50it/s]\n",
      "100%|██████████| 831/831 [01:05<00:00, 12.77it/s]\n",
      "[2025-05-07 01:59:57,674]::1259713826::DEBUG::Epoch: 4 , 1-MAE: 0.9073 , 1-Training Loss: 0.9806 , 1-val_avg_o_loss:0.8980 , 1-val_avg_c_loss:0.9050 , 1-val_avg_e_loss:0.8921 , 1-val_avg_a_loss:0.9371 , 1-val_avg_n_loss:0.9041\n",
      "100%|██████████| 2262/2262 [10:54<00:00,  3.46it/s]\n",
      "100%|██████████| 831/831 [02:02<00:00,  6.78it/s]\n",
      "[2025-05-07 02:12:56,094]::1259713826::DEBUG::Epoch: 5 , 1-MAE: 0.9076 , 1-Training Loss: 0.9834 , 1-val_avg_o_loss:0.8968 , 1-val_avg_c_loss:0.9076 , 1-val_avg_e_loss:0.8906 , 1-val_avg_a_loss:0.9366 , 1-val_avg_n_loss:0.9065\n",
      "100%|██████████| 2262/2262 [13:26<00:00,  2.80it/s]\n",
      "100%|██████████| 831/831 [02:03<00:00,  6.73it/s]\n",
      "[2025-05-07 02:28:27,192]::1259713826::DEBUG::Epoch: 6 , 1-MAE: 0.9069 , 1-Training Loss: 0.9851 , 1-val_avg_o_loss:0.8980 , 1-val_avg_c_loss:0.9076 , 1-val_avg_e_loss:0.8900 , 1-val_avg_a_loss:0.9381 , 1-val_avg_n_loss:0.9008\n",
      "100%|██████████| 2262/2262 [13:29<00:00,  2.80it/s]\n",
      "100%|██████████| 831/831 [02:02<00:00,  6.79it/s]\n",
      "[2025-05-07 02:44:00,587]::1259713826::DEBUG::Epoch: 7 , 1-MAE: 0.9081 , 1-Training Loss: 0.9863 , 1-val_avg_o_loss:0.8972 , 1-val_avg_c_loss:0.9060 , 1-val_avg_e_loss:0.8921 , 1-val_avg_a_loss:0.9364 , 1-val_avg_n_loss:0.9089\n",
      "100%|██████████| 2262/2262 [13:30<00:00,  2.79it/s]\n",
      "100%|██████████| 831/831 [02:04<00:00,  6.69it/s]\n",
      "[2025-05-07 02:59:37,663]::1259713826::DEBUG::Epoch: 8 , 1-MAE: 0.9082 , 1-Training Loss: 0.9875 , 1-val_avg_o_loss:0.8994 , 1-val_avg_c_loss:0.9045 , 1-val_avg_e_loss:0.8951 , 1-val_avg_a_loss:0.9363 , 1-val_avg_n_loss:0.9054\n",
      "100%|██████████| 2262/2262 [13:23<00:00,  2.81it/s]\n",
      "100%|██████████| 831/831 [02:03<00:00,  6.75it/s]\n",
      "[2025-05-07 03:15:05,831]::1259713826::DEBUG::Epoch: 9 , 1-MAE: 0.9078 , 1-Training Loss: 0.9884 , 1-val_avg_o_loss:0.8973 , 1-val_avg_c_loss:0.9081 , 1-val_avg_e_loss:0.8914 , 1-val_avg_a_loss:0.9362 , 1-val_avg_n_loss:0.9059\n",
      "100%|██████████| 2262/2262 [13:18<00:00,  2.83it/s]\n",
      "100%|██████████| 831/831 [02:02<00:00,  6.77it/s]\n",
      "[2025-05-07 03:30:28,636]::1259713826::DEBUG::Epoch: 10 , 1-MAE: 0.9095 , 1-Training Loss: 0.9892 , 1-val_avg_o_loss:0.8988 , 1-val_avg_c_loss:0.9089 , 1-val_avg_e_loss:0.8945 , 1-val_avg_a_loss:0.9356 , 1-val_avg_n_loss:0.9094\n"
     ]
    }
   ],
   "source": [
    "max_value = float('inf')\n",
    "for i in range(start_epoch, epochs):\n",
    "    train_avg_loss = 0\n",
    "    val_avg_loss = 0\n",
    "    train_avg_o_loss = 0\n",
    "    train_avg_c_loss = 0\n",
    "    train_avg_e_loss = 0\n",
    "    train_avg_a_loss = 0\n",
    "    train_avg_n_loss = 0\n",
    "    val_avg_o_loss = 0\n",
    "    val_avg_c_loss = 0\n",
    "    val_avg_e_loss = 0\n",
    "    val_avg_a_loss = 0\n",
    "    val_avg_n_loss = 0\n",
    "    for fullshot,big_five_data in tqdm(train_dataloader):\n",
    "        fullshot=fullshot.permute(0,4,1,2,3)\n",
    "        big_five_data=big_five_data.permute(0,2,1)\n",
    "        big_five_data=big_five_data.squeeze()\n",
    "        fullshot = fullshot.to(device)\n",
    "        big_five_data = big_five_data.to(device)\n",
    "        optimizer.zero_grad()  # 기울기가 0이 됩니다.\n",
    "        hypothesis = model(fullshot) # 모델의 예측 결과를 저장합니다.\n",
    "        loss = criterion(hypothesis, big_five_data)  # 예측된 결과와 실제 태그 사이의 손실 값을 저장합니다.\n",
    "        loss.backward()  # 역방향 전파입니다. \n",
    "        optimizer.step()  \n",
    "        train_avg_loss += loss  \n",
    "    train_avg_loss=train_avg_loss/len(train_dataloader)\n",
    "    with torch.no_grad():#validate\n",
    "        for fullshot,big_five_data in tqdm(val_dataloader):\n",
    "            fullshot=fullshot.permute(0,4,1,2,3)\n",
    "            big_five_data=big_five_data.permute(0,2,1)\n",
    "            big_five_data=big_five_data.squeeze()\n",
    "            fullshot = fullshot.to(device)\n",
    "            big_five_data = big_five_data.to(device)\n",
    "            hypothesis = model(fullshot)\n",
    "            val_loss = criterion(hypothesis, big_five_data)\n",
    "            hypothesiso,hypothesisc,hypothesise,hypothesisa,hypothesisn=extract_ocean(hypothesis,len(fullshot))\n",
    "            big_five_datao,big_five_datac,big_five_datae,big_five_dataa,big_five_datan=extract_ocean(big_five_data,len(fullshot))\n",
    "            val_o_loss = criterion(hypothesiso, big_five_datao) \n",
    "            val_c_loss = criterion(hypothesisc, big_five_datac) \n",
    "            val_e_loss = criterion(hypothesise, big_five_datae) \n",
    "            val_a_loss = criterion(hypothesisa, big_five_dataa) \n",
    "            val_n_loss = criterion(hypothesisn, big_five_datan)  \n",
    "            val_avg_loss += val_loss\n",
    "            val_avg_o_loss += val_o_loss\n",
    "            val_avg_c_loss += val_c_loss\n",
    "            val_avg_e_loss += val_e_loss\n",
    "            val_avg_a_loss += val_a_loss\n",
    "            val_avg_n_loss += val_n_loss\n",
    "        val_avg_loss=val_avg_loss/len(val_dataloader)\n",
    "        val_avg_o_loss=val_avg_o_loss/len(val_dataloader)\n",
    "        val_avg_c_loss=val_avg_c_loss/len(val_dataloader)\n",
    "        val_avg_e_loss=val_avg_e_loss/len(val_dataloader)\n",
    "        val_avg_a_loss=val_avg_a_loss/len(val_dataloader)\n",
    "        val_avg_n_loss=val_avg_n_loss/len(val_dataloader)\n",
    "    torch.cuda.empty_cache()\n",
    "    start_epoch+=1\n",
    "    if val_avg_loss < max_value:\n",
    "        max_value=val_avg_loss\n",
    "        torch.save({\n",
    "            'epoch': i+1,\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'loss': val_avg_loss,\n",
    "        }, save_model_file_path.format('model',start_epoch,'pth'))\n",
    "    logger.debug('Epoch: {} , 1-MAE: {:.4f} , 1-Training Loss: {:.4f} , 1-val_avg_o_loss:{:.4f} , 1-val_avg_c_loss:{:.4f} , 1-val_avg_e_loss:{:.4f} , 1-val_avg_a_loss:{:.4f} , 1-val_avg_n_loss:{:.4f}'.format(i+1, 1-val_avg_loss, 1-train_avg_loss,1-val_avg_o_loss, 1-val_avg_c_loss, 1-val_avg_e_loss, 1-val_avg_a_loss,1-val_avg_n_loss))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
