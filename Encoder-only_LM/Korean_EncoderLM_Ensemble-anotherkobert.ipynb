{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c64b256",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-01T04:37:08.557689Z",
     "start_time": "2023-01-01T04:37:06.343746Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, TrainerCallback, DefaultDataCollator\n",
    "from transformers.trainer_pt_utils import _get_learning_rate\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
    "import logging\n",
    "\n",
    "logging.disable(logging.INFO) # disable INFO and DEBUG logging everywhere\n",
    "logging.disable(logging.WARNING) # disable WARNING, INFO and DEBUG logging everywhere\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c096ae94",
   "metadata": {},
   "source": [
    "## Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c24ef8fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-01T04:37:08.676384Z",
     "start_time": "2023-01-01T04:37:08.560371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed:int = 1004):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # current gpu seed\n",
    "    torch.cuda.manual_seed_all(seed) # All gpu seed\n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "    torch.backends.cudnn.benchmark = False  # True로 하면 gpu에 적합한 알고리즘을 선택함.\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d883de",
   "metadata": {},
   "source": [
    "### Config & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db325225",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-01T04:37:08.691558Z",
     "start_time": "2023-01-01T04:37:08.677811Z"
    }
   },
   "outputs": [],
   "source": [
    "KOELECTRA = 'monologg/koelectra-base-v3-discriminator'\n",
    "KOBIGBIRD = 'monologg/kobigbird-bert-base'\n",
    "KOBERT = 'monologg/kobert'\n",
    "KOROBERTA = 'klue/roberta-base'\n",
    "KOROBERTA_checkpoint = '/mnt/HDD8TB/PersonalityAI/koRoBERTa_base-checkpoint16000'\n",
    "\n",
    "# Hidden_size = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8de1fda6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-01T04:37:15.127843Z",
     "start_time": "2023-01-01T04:37:09.544041Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for monologg/kobert contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/monologg/kobert.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    }
   ],
   "source": [
    "electra_tokenizer = AutoTokenizer.from_pretrained(KOELECTRA)\n",
    "bigbird_tokenizer = AutoTokenizer.from_pretrained(KOBIGBIRD)\n",
    "klue_tokenizer = AutoTokenizer.from_pretrained(KOROBERTA)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(KOBERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6e6e58-e90a-4915-8f4d-8deb77d29dc7",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b2c812e-f09e-4c51-8e71-e87f93b7840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, label, electra_tokenizer, bigbird_tokenizer, klue_tokenizer, bert_tokenizer):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.electra_tokenizer = electra_tokenizer\n",
    "        self.bigbird_tokenizer = bigbird_tokenizer\n",
    "        self.klue_tokenizer = klue_tokenizer\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        electra_tokens = self.electra_tokenizer(text, \n",
    "                              #  return_tensors=\"pt\",  # pytorch.Tensor로 리턴\n",
    "                                max_length=512, \n",
    "                                padding=\"max_length\",  \n",
    "                                truncation=True,  # max_length 넘어가면 버림)\n",
    "                               )\n",
    "        bigbird_tokens = self.bigbird_tokenizer(text, \n",
    "                              #  return_tensors=\"pt\",  # pytorch.Tensor로 리턴\n",
    "                                max_length=512, \n",
    "                                padding=\"max_length\",  \n",
    "                                truncation=True,  # max_length 넘어가면 버림)\n",
    "                               )\n",
    "        klue_tokens = self.klue_tokenizer(text, \n",
    "                              #  return_tensors=\"pt\",  # pytorch.Tensor로 리턴\n",
    "                                max_length=512, \n",
    "                                padding=\"max_length\",  \n",
    "                                truncation=True,  # max_length 넘어가면 버림)\n",
    "                               )\n",
    "        bert_tokens = self.bert_tokenizer(text, \n",
    "                              #  return_tensors=\"pt\",  # pytorch.Tensor로 리턴\n",
    "                                max_length=512, \n",
    "                                padding=\"max_length\",  \n",
    "                                truncation=True,  # max_length 넘어가면 버림)\n",
    "                               )\n",
    "        \n",
    "        for key in list(bigbird_tokens.keys()):\n",
    "            bigbird_tokens[key+\"_bird\"] = bigbird_tokens.pop(key)\n",
    "\n",
    "        for key in list(klue_tokens.keys()):\n",
    "            klue_tokens[key+\"_klue\"] = klue_tokens.pop(key)\n",
    "\n",
    "        for key in list(bert_tokens.keys()):\n",
    "            bert_tokens[key+\"_bert\"] = bert_tokens.pop(key)\n",
    "\n",
    "        electra_tokens.update(bigbird_tokens)\n",
    "        electra_tokens.update(klue_tokens)\n",
    "        electra_tokens.update(bert_tokens)\n",
    "\n",
    "        electra_tokens['label'] = [float(i) for i in self.label[idx][1:-1].split(',')]\n",
    "        \n",
    "     #   item = {key: torch.tensor(values) for key, values in electra_tokens.items()}\n",
    "\n",
    "        return electra_tokens\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9fe8033-9d91-4f60-9ed3-4bfafecb5ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "keti_train = pd.read_csv(\"/mnt/HDD8TB/Data/PAI/KETI_DATASET/KETI_train_from_ver0.4.csv\",\n",
    "                 index_col=0\n",
    "                 )\n",
    "keti_val = pd.read_csv(\"/mnt/HDD8TB/Data/PAI/KETI_DATASET/KETI_val_from_ver0.4.csv\",\n",
    "                 index_col=0\n",
    "                 )\n",
    "keti_test = pd.read_csv(\"/mnt/HDD8TB/Data/PAI/KETI_DATASET/KETI_test_from_ver0.4.csv\",\n",
    "                 index_col=0\n",
    "                 )\n",
    "#data = data.sample(frac=1).reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d10f85d4-ed5a-4fc1-aea2-79dea5873c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "keti_train['len'] = keti_train['transcription'].str.split().apply(len)\n",
    "keti_val['len'] = keti_val['transcription'].str.split().apply(len)\n",
    "keti_test['len'] = keti_test['transcription'].str.split().apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec2c10a0-395e-42dc-9d31-b3175ac197b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    631.000000\n",
       "mean      12.036450\n",
       "std       11.124131\n",
       "min        1.000000\n",
       "25%        6.000000\n",
       "50%        8.000000\n",
       "75%       14.000000\n",
       "max       77.000000\n",
       "Name: len, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keti_test.len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "496140fe-1d66-4d77-9e4f-cc615a554e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CLIPDataset(data = keti_train['transcription'].to_list(),\n",
    "                           label = keti_train['OCEAN'].to_list(),\n",
    "                            electra_tokenizer = electra_tokenizer,\n",
    "                            bigbird_tokenizer = bigbird_tokenizer,\n",
    "                            klue_tokenizer = klue_tokenizer,\n",
    "                            bert_tokenizer = bert_tokenizer\n",
    "                           )\n",
    "\n",
    "val_dataset = CLIPDataset(data = keti_val['transcription'].to_list(),\n",
    "                           label = keti_val['OCEAN'].to_list(),\n",
    "                          electra_tokenizer = electra_tokenizer,\n",
    "                          bigbird_tokenizer = bigbird_tokenizer,\n",
    "                          klue_tokenizer = klue_tokenizer,\n",
    "                          bert_tokenizer = bert_tokenizer\n",
    "                         )\n",
    "\n",
    "test_dataset = CLIPDataset(data = keti_test['transcription'].to_list(),\n",
    "                           label = keti_test['OCEAN'].to_list(),\n",
    "                           electra_tokenizer = electra_tokenizer,\n",
    "                           bigbird_tokenizer = bigbird_tokenizer,\n",
    "                           klue_tokenizer = klue_tokenizer,\n",
    "                           bert_tokenizer = bert_tokenizer\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33ea8e9e-1ba9-4b74-aac6-9fb341a2cc32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4431\n",
      "{'input_ids': [2, 6258, 4219, 2279, 4116, 4116, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids_bird': [2, 18733, 19856, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids_bird': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask_bird': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids_klue': [0, 14455, 15433, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type_ids_klue': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask_klue': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids_bert': [2, 3121, 5439, 1469, 5702, 5702, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type_ids_bert': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask_bert': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': [0.5, 0.4375, 0.4375, 0.7291666666666666, 0.45833333333333337]}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.__len__())\n",
    "print(train_dataset.__getitem__(970))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b21b958-2e67-42de-8067-58ce302605f1",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12401a4d-8ad3-48f0-bf84-b293b20301b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "koelectra size :  112330752\n",
      "kobert size :  110617344\n",
      "kobigbird size :  113753856\n",
      "koroberta size :  110618112\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(KOELECTRA)\n",
    "model2 = AutoModel.from_pretrained(\"klue/bert-base\")\n",
    "model3 = AutoModel.from_pretrained(KOBIGBIRD)\n",
    "model4 = AutoModel.from_pretrained(KOROBERTA_checkpoint)\n",
    "\n",
    "print(\"koelectra size : \", model.num_parameters())\n",
    "print(\"kobert size : \", model2.num_parameters())\n",
    "print(\"kobigbird size : \", model3.num_parameters())\n",
    "print(\"koroberta size : \", model4.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "901898d4-1b48-4a96-b533-7c321ae20bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, num_labels, projection_dim=768, d_out=0.1, ):\n",
    "        super().__init__()\n",
    "        self.projection_dim = projection_dim\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.koelectra = AutoModel.from_pretrained(KOELECTRA).eval()\n",
    "        self.kobigbird = AutoModel.from_pretrained(KOBIGBIRD).eval()\n",
    "        self.kobert = AutoModel.from_pretrained(KOBERT).eval()\n",
    "        self.koroberta = AutoModel.from_pretrained(KOROBERTA_checkpoint).eval()  # 1 x (768 *4) <행렬곱> (768*4) x 5 == 1 x 5\n",
    "        \n",
    "        self.dense = nn.Linear(self.kobert.config.hidden_size * 4, self.projection_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.dropout = nn.Dropout(d_out)\n",
    "        self.out_proj = nn.Linear(self.projection_dim, self.num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids = None, attention_mask = None, token_type_ids = None,\n",
    "        input_ids_bird = None, attention_mask_bird = None, token_type_ids_bird = None,\n",
    "        input_ids_klue = None, attention_mask_klue = None, token_type_ids_klue = None,\n",
    "        input_ids_bert = None, attention_mask_bert = None, token_type_ids_bert = None,\n",
    "        position_ids = None,\n",
    "        head_mask = None,\n",
    "        inputs_embeds = None,\n",
    "        labels = None,\n",
    "        output_attentions = None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict = None,\n",
    "    ):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            electra_outputs = self.koelectra(\n",
    "                input_ids = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                token_type_ids = token_type_ids,\n",
    "                output_attentions = output_attentions,\n",
    "                output_hidden_states = output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "\n",
    "            bigbird_outputs = self.kobigbird(\n",
    "                input_ids = input_ids_bird,\n",
    "                attention_mask = attention_mask_bird,\n",
    "                token_type_ids = token_type_ids_bird,\n",
    "                output_attentions = output_attentions,\n",
    "                output_hidden_states = output_hidden_states,\n",
    "                return_dict = return_dict,\n",
    "            )\n",
    "\n",
    "            bert_outputs = self.kobert(\n",
    "                input_ids = input_ids_bert,\n",
    "                attention_mask = attention_mask_bert,\n",
    "                token_type_ids = token_type_ids_bert,\n",
    "                output_attentions = output_attentions,\n",
    "                output_hidden_states = output_hidden_states,\n",
    "                return_dict = return_dict,\n",
    "            )\n",
    "\n",
    "            roberta_outputs = self.koroberta(\n",
    "                input_ids = input_ids_klue,\n",
    "                attention_mask = attention_mask_klue,\n",
    "                token_type_ids = token_type_ids_klue,\n",
    "                output_attentions = output_attentions,\n",
    "                output_hidden_states = output_hidden_states,\n",
    "                return_dict = return_dict,\n",
    "            )\n",
    "        \n",
    "        # get pooled output([CLS] Token)\n",
    "        electra_embeds = electra_outputs[0][:,0,:]\n",
    "        bigbird_embeds = bigbird_outputs[0][:,0,:]\n",
    "        bert_embeds = bert_outputs[0][:,0,:]\n",
    "        roberta_embeds = roberta_outputs[0][:,0,:]\n",
    "\n",
    "        concated_input = torch.concat([electra_embeds, bigbird_embeds, bert_embeds, roberta_embeds], dim=1)  # [Batch_size, hidden * 4 (3072)] \n",
    "\n",
    "     #   avg_input = (electra_embeds + bigbird_embeds + bert_embeds + roberta_embeds) / 4\n",
    "\n",
    "        x = self.dense(concated_input)\n",
    "        x = self.act(x)  # GELU\n",
    "        x = self.dropout(x)\n",
    "        logits = self.out_proj(x)  # [Batch_Size x 5]\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if labels != None:\n",
    "            loss_fct = nn.L1Loss()\n",
    "    \n",
    "           # loss_fct = loss_fct(logits_O.view(-1, self.num_labels), labels[0])\n",
    "            loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "604076dc-a091-48bd-a321-a239d8094f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnsembleModel(num_labels = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c19b37e-52f8-419b-bc18-4c961d29ee96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "431253509"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61eedc6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "EnsembleModel                                                     --\n",
       "├─ElectraModel: 1-1                                               --\n",
       "│    └─ElectraEmbeddings: 2-1                                     --\n",
       "│    │    └─Embedding: 3-1                                        26,880,000\n",
       "│    │    └─Embedding: 3-2                                        393,216\n",
       "│    │    └─Embedding: 3-3                                        1,536\n",
       "│    │    └─LayerNorm: 3-4                                        1,536\n",
       "│    │    └─Dropout: 3-5                                          --\n",
       "│    └─ElectraEncoder: 2-2                                        --\n",
       "│    │    └─ModuleList: 3-6                                       85,054,464\n",
       "├─BigBirdModel: 1-2                                               --\n",
       "│    └─BigBirdEmbeddings: 2-3                                     --\n",
       "│    │    └─Embedding: 3-7                                        24,960,000\n",
       "│    │    └─Embedding: 3-8                                        3,145,728\n",
       "│    │    └─Embedding: 3-9                                        1,536\n",
       "│    │    └─LayerNorm: 3-10                                       1,536\n",
       "│    │    └─Dropout: 3-11                                         --\n",
       "│    └─BigBirdEncoder: 2-4                                        --\n",
       "│    │    └─ModuleList: 3-12                                      85,054,464\n",
       "│    └─Linear: 2-5                                                590,592\n",
       "│    └─Tanh: 2-6                                                  --\n",
       "├─BertModel: 1-3                                                  --\n",
       "│    └─BertEmbeddings: 2-7                                        --\n",
       "│    │    └─Embedding: 3-13                                       6,145,536\n",
       "│    │    └─Embedding: 3-14                                       393,216\n",
       "│    │    └─Embedding: 3-15                                       1,536\n",
       "│    │    └─LayerNorm: 3-16                                       1,536\n",
       "│    │    └─Dropout: 3-17                                         --\n",
       "│    └─BertEncoder: 2-8                                           --\n",
       "│    │    └─ModuleList: 3-18                                      85,054,464\n",
       "│    └─BertPooler: 2-9                                            --\n",
       "│    │    └─Linear: 3-19                                          590,592\n",
       "│    │    └─Tanh: 3-20                                            --\n",
       "├─RobertaModel: 1-4                                               --\n",
       "│    └─RobertaEmbeddings: 2-10                                    --\n",
       "│    │    └─Embedding: 3-21                                       24,576,000\n",
       "│    │    └─Embedding: 3-22                                       394,752\n",
       "│    │    └─Embedding: 3-23                                       768\n",
       "│    │    └─LayerNorm: 3-24                                       1,536\n",
       "│    │    └─Dropout: 3-25                                         --\n",
       "│    └─RobertaEncoder: 2-11                                       --\n",
       "│    │    └─ModuleList: 3-26                                      85,054,464\n",
       "│    └─RobertaPooler: 2-12                                        --\n",
       "│    │    └─Linear: 3-27                                          590,592\n",
       "│    │    └─Tanh: 3-28                                            --\n",
       "├─Linear: 1-5                                                     2,360,064\n",
       "├─GELU: 1-6                                                       --\n",
       "├─Dropout: 1-7                                                    --\n",
       "├─Linear: 1-8                                                     3,845\n",
       "==========================================================================================\n",
       "Total params: 431,253,509\n",
       "Trainable params: 431,253,509\n",
       "Non-trainable params: 0\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadcfb85",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "071a8c2b-0b73-43a0-8925-8fb16abcfc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from transformers import EvalPrediction\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import torch\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "    \n",
    "    MAE = 1 - mean_absolute_error(labels, preds)\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        '1 - MAE' : MAE\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a313320",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-01T04:42:03.156904Z",
     "start_time": "2023-01-01T04:42:00.570159Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/egg2018037024/venv/lib/python3.8/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/mnt/HDD8TB/PersonalityAI/FI_1head/keti_koEnsemble_model_usingrobertacheck+kobert_concat_nosafetensor2\",\n",
    "    logging_dir= \"/mnt/HDD8TB/PersonalityAI/FI_1head/keti_koEnsemble_model_usingrobertacheck+kobert_concat_nosafetensor2_log\",\n",
    "    num_train_epochs=10,\n",
    "    learning_rate = 3e-4,  # Best : 3e-4\n",
    "   # max_steps=1000,\n",
    "    per_device_train_batch_size=32,\n",
    "#    gradient_accumulation_steps = 16,\n",
    "    per_device_eval_batch_size = 32,\n",
    "#    eval_accumulation_steps = 32,\n",
    "    logging_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    dataloader_num_workers = 12,\n",
    "#    warmup_ratio = 0.1,\n",
    "#    weight_decay=0.01,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_safetensors =False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=DefaultDataCollator(return_tensors = \"pt\"),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247c35d7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-01-01T04:37:06.801Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trainer.add_callback(CustomCallback(trainer))\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74266940-c4a3-445c-a086-58c9cbe7b974",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53d53d70-ea93-489c-8c39-fe9a29c8e6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ensemble = EnsembleModel(num_labels = 5)\n",
    "\n",
    "state_dict = torch.load(\"/mnt/HDD8TB/PersonalityAI/FI_1head/keti_koEnsemble_model_usingrobertacheck+kobert_concat_nosafetensor2/checkpoint-70/pytorch_model.bin\")\n",
    "\n",
    "test_ensemble.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2239899a-ad87-4aef-9deb-0452c533da5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5553, 0.6893, 0.7340, 0.5556, 0.4327]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "jf_text1 = \"한밤중에 깨어나면 베개에 바퀴벌레가 '많아' 있고 이불이 매우 불편합니다! \\\n",
    "집에 돌아와서 짐을 풀어보니 두 개를 가지고 왔다는 걸 발견했어요! 역겨운! ! !\"\n",
    "\n",
    "class testDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, electra_tokenizer, bigbird_tokenizer, klue_tokenizer, bert_tokenizer, label=None):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.electra_tokenizer = electra_tokenizer\n",
    "        self.bigbird_tokenizer = bigbird_tokenizer\n",
    "        self.klue_tokenizer = klue_tokenizer\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        electra_tokens = self.electra_tokenizer(text, \n",
    "                                return_tensors=\"pt\",  # pytorch.Tensor로 리턴\n",
    "                                max_length=512, \n",
    "                                padding=\"max_length\",  \n",
    "                                truncation=True,  # max_length 넘어가면 버림)\n",
    "                               )\n",
    "        bigbird_tokens = self.bigbird_tokenizer(text, \n",
    "                                return_tensors=\"pt\",  # pytorch.Tensor로 리턴\n",
    "                                max_length=512, \n",
    "                                padding=\"max_length\",  \n",
    "                                truncation=True,  # max_length 넘어가면 버림)\n",
    "                               )\n",
    "        klue_tokens = self.klue_tokenizer(text, \n",
    "                                return_tensors=\"pt\",  # pytorch.Tensor로 리턴\n",
    "                                max_length=512, \n",
    "                                padding=\"max_length\",  \n",
    "                                truncation=True,  # max_length 넘어가면 버림)\n",
    "                               )\n",
    "        bert_tokens = self.bert_tokenizer(text, \n",
    "                                return_tensors=\"pt\",  # pytorch.Tensor로 리턴\n",
    "                                max_length=512, \n",
    "                                padding=\"max_length\",  \n",
    "                                truncation=True,  # max_length 넘어가면 버림)\n",
    "                               )\n",
    "        \n",
    "        for key in list(bigbird_tokens.keys()):\n",
    "            bigbird_tokens[key+\"_bird\"] = bigbird_tokens.pop(key)\n",
    "\n",
    "        for key in list(klue_tokens.keys()):\n",
    "            klue_tokens[key+\"_klue\"] = klue_tokens.pop(key)\n",
    "\n",
    "        for key in list(bert_tokens.keys()):\n",
    "            bert_tokens[key+\"_bert\"] = bert_tokens.pop(key)\n",
    "\n",
    "        electra_tokens.update(bigbird_tokens)\n",
    "        electra_tokens.update(klue_tokens)\n",
    "        electra_tokens.update(bert_tokens)\n",
    "        \n",
    "     #   item = {key: torch.tensor(values) for key, values in electra_tokens.items()}\n",
    "\n",
    "        return electra_tokens\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "tokens = testDataset(data = jf_text1,\n",
    "                            electra_tokenizer = electra_tokenizer,\n",
    "                            bigbird_tokenizer = bigbird_tokenizer,\n",
    "                            klue_tokenizer = klue_tokenizer,\n",
    "                            bert_tokenizer = bert_tokenizer,\n",
    "                           )\n",
    "tokens = tokens.__getitem__(0)\n",
    "\n",
    "_, result = test_ensemble(\n",
    "        input_ids = tokens['input_ids'],\n",
    "        attention_mask = tokens['attention_mask'],\n",
    "        token_type_ids = tokens['token_type_ids'],\n",
    "        input_ids_bird = tokens['input_ids_bird'],\n",
    "        attention_mask_bird = tokens['attention_mask_bird'],\n",
    "        token_type_ids_bird = tokens['token_type_ids_bird'],\n",
    "        input_ids_klue = tokens['input_ids_klue'],\n",
    "        attention_mask_klue = tokens['attention_mask_klue'],\n",
    "        token_type_ids_klue = tokens['token_type_ids_klue'],\n",
    "        input_ids_bert = tokens['input_ids_bert'],\n",
    "        attention_mask_bert = tokens['attention_mask_bert'],\n",
    "        token_type_ids_bert = tokens['token_type_ids_bert'],\n",
    "            )\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d6ce546-25db-4c70-884c-8fce48406550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5552600622177124"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13383a63-8f83-4da3-90ec-8d9adb3625d9",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6bffd09-1915-4a85-9933-69ffa98eba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "    \n",
    "    labels = labels.T\n",
    "    preds = preds.T\n",
    "    \n",
    "    MAE = []\n",
    "    \n",
    "    for i in range(5):\n",
    "        MAE.append(1 - mean_absolute_error(labels[i], preds[i]))\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'Avg' : np.mean(MAE),\n",
    "        '1 - MAE' : MAE\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9e1c9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set(), [])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from safetensors.torch import load_model\n",
    "\n",
    "test_model = EnsembleModel(num_labels = 5)\n",
    "\n",
    "######################################### Check your model path Again#####################################\n",
    "load_model(test_model, \"/mnt/HDD8TB/PersonalityAI/FI_1head/keti_koEnsemble_model_usingrobertacheck+kobert_concat/checkpoint-70/model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "779daedb-1d7e-45e6-82ae-09b0acc02b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_trainer = Trainer(\n",
    "    model=test_ensemble,\n",
    "    args=training_args,\n",
    "    data_collator=DefaultDataCollator(return_tensors = \"pt\"),\n",
    "    compute_metrics=test_compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b403703-3f0f-4b98-b17a-42ec14d2a68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.1510709971189499,\n",
       " 'eval_Avg': 0.8489405632019043,\n",
       " 'eval_1 - MAE': [0.8426289558410645,\n",
       "  0.836871549487114,\n",
       "  0.8657149076461792,\n",
       "  0.8616010397672653,\n",
       "  0.8378863632678986],\n",
       " 'eval_runtime': 12.0291,\n",
       " 'eval_samples_per_second': 52.456,\n",
       " 'eval_steps_per_second': 0.831}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2be882f-be0a-450e-84fa-c3d535911bab",
   "metadata": {},
   "source": [
    "### Test another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74baebc8-ea83-4943-9dc1-4373a5606776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/egg2018037024/venv/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "KOELECTRA = 'monologg/koelectra-base-v3-discriminator'\n",
    "KOBIGBIRD = 'monologg/kobigbird-bert-base'\n",
    "KOBERT = 'monologg/kobert'\n",
    "KOROBERTA = 'klue/roberta-base'\n",
    "KOROBERTA_checkpoint = '/mnt/HDD8TB/PersonalityAI/koRoBERTa_base-checkpoint16000'\n",
    "\n",
    "\n",
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, electra_tokenizer, bigbird_tokenizer, klue_tokenizer, bert_tokenizer, label=None):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.electra_tokenizer = electra_tokenizer\n",
    "        self.bigbird_tokenizer = bigbird_tokenizer\n",
    "        self.klue_tokenizer = klue_tokenizer\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data\n",
    "        electra_tokens = self.electra_tokenizer(text, \n",
    "                                return_tensors=\"pt\",  # pytorch.Tensor로 리턴\n",
    "                                max_length=512, \n",
    "                                padding=\"max_length\",  \n",
    "                                truncation=True,  # max_length 넘어가면 버림)\n",
    "                               )\n",
    "        bigbird_tokens = self.bigbird_tokenizer(text, \n",
    "                                return_tensors=\"pt\",  # pytorch.Tensor로 리턴\n",
    "                                max_length=512, \n",
    "                                padding=\"max_length\",  \n",
    "                                truncation=True,  # max_length 넘어가면 버림)\n",
    "                               )\n",
    "        klue_tokens = self.klue_tokenizer(text, \n",
    "                                return_tensors=\"pt\",  # pytorch.Tensor로 리턴\n",
    "                                max_length=512, \n",
    "                                padding=\"max_length\",  \n",
    "                                truncation=True,  # max_length 넘어가면 버림)\n",
    "                               )\n",
    "        bert_tokens = self.bert_tokenizer(text, \n",
    "                                return_tensors=\"pt\",  # pytorch.Tensor로 리턴\n",
    "                                max_length=512, \n",
    "                                padding=\"max_length\",  \n",
    "                                truncation=True,  # max_length 넘어가면 버림)\n",
    "                               )\n",
    "        \n",
    "        for key in list(bigbird_tokens.keys()):\n",
    "            bigbird_tokens[key+\"_bird\"] = bigbird_tokens.pop(key)\n",
    "\n",
    "        for key in list(klue_tokens.keys()):\n",
    "            klue_tokens[key+\"_klue\"] = klue_tokens.pop(key)\n",
    "\n",
    "        for key in list(bert_tokens.keys()):\n",
    "            bert_tokens[key+\"_bert\"] = bert_tokens.pop(key)\n",
    "\n",
    "        electra_tokens.update(bigbird_tokens)\n",
    "        electra_tokens.update(klue_tokens)\n",
    "        electra_tokens.update(bert_tokens)\n",
    "        \n",
    "     #   item = {key: torch.tensor(values) for key, values in electra_tokens.items()}\n",
    "\n",
    "        return electra_tokens\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, num_labels, projection_dim=768, d_out=0.1, ):\n",
    "        super().__init__()\n",
    "        self.projection_dim = projection_dim\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.koelectra = AutoModel.from_pretrained(KOELECTRA).eval()\n",
    "        self.kobigbird = AutoModel.from_pretrained(KOBIGBIRD).eval()\n",
    "        self.kobert = AutoModel.from_pretrained(KOBERT).eval()\n",
    "        self.koroberta = AutoModel.from_pretrained(KOROBERTA_checkpoint).eval()  # 1 x (768 *4) <행렬곱> (768*4) x 5 == 1 x 5\n",
    "        \n",
    "        self.dense = nn.Linear(self.kobert.config.hidden_size * 4, self.projection_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.dropout = nn.Dropout(d_out)\n",
    "        self.out_proj = nn.Linear(self.projection_dim, self.num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids = None, attention_mask = None, token_type_ids = None,\n",
    "        input_ids_bird = None, attention_mask_bird = None, token_type_ids_bird = None,\n",
    "        input_ids_klue = None, attention_mask_klue = None, token_type_ids_klue = None,\n",
    "        input_ids_bert = None, attention_mask_bert = None, token_type_ids_bert = None,\n",
    "        position_ids = None, position_ids_bird = None, position_ids_klue = None, position_ids_bert = None,\n",
    "        head_mask = None,\n",
    "        inputs_embeds = None,\n",
    "        labels = None,\n",
    "        output_attentions = None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict = None,\n",
    "    ):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            electra_outputs = self.koelectra(\n",
    "                input_ids = input_ids,\n",
    "                attention_mask = attention_mask,\n",
    "                token_type_ids = token_type_ids,\n",
    "                output_attentions = output_attentions,\n",
    "                output_hidden_states = output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "\n",
    "            bigbird_outputs = self.kobigbird(\n",
    "                input_ids = input_ids_bird,\n",
    "                attention_mask = attention_mask_bird,\n",
    "                token_type_ids = token_type_ids_bird,\n",
    "                output_attentions = output_attentions,\n",
    "                output_hidden_states = output_hidden_states,\n",
    "                return_dict = return_dict,\n",
    "            )\n",
    "\n",
    "            bert_outputs = self.kobert(\n",
    "                input_ids = input_ids_bert,\n",
    "                attention_mask = attention_mask_bert,\n",
    "                token_type_ids = token_type_ids_bert,\n",
    "                output_attentions = output_attentions,\n",
    "                output_hidden_states = output_hidden_states,\n",
    "                return_dict = return_dict,\n",
    "            )\n",
    "\n",
    "            roberta_outputs = self.koroberta(\n",
    "                input_ids = input_ids_klue,\n",
    "                attention_mask = attention_mask_klue,\n",
    "                token_type_ids = token_type_ids_klue,\n",
    "                output_attentions = output_attentions,\n",
    "                output_hidden_states = output_hidden_states,\n",
    "                return_dict = return_dict,\n",
    "            )\n",
    "\n",
    "        print(electra_outputs[0].shape)\n",
    "        \n",
    "        # get pooled output([CLS] Token)\n",
    "        electra_embeds = electra_outputs[0][:,0,:]\n",
    "        bigbird_embeds = bigbird_outputs[0][:,0,:]\n",
    "        bert_embeds = bert_outputs[0][:,0,:]\n",
    "        roberta_embeds = roberta_outputs[0][:,0,:]\n",
    "\n",
    "        print(\"elec shape: \", electra_embeds.shape)\n",
    "        print(\"bigbird shape: \", bigbird_embeds.shape)\n",
    "        print(\"bert shape: \", bert_embeds.shape)\n",
    "        print(\"roberta shape: \", roberta_embeds.shape)\n",
    "\n",
    "        concated_input = torch.cat([electra_embeds, bigbird_embeds, bert_embeds, roberta_embeds], dim=1)  # [Batch_size, hidden * 4 (3072)] \n",
    "\n",
    "     #   avg_input = (electra_embeds + bigbird_embeds + bert_embeds + roberta_embeds) / 4\n",
    "\n",
    "        x = self.dense(concated_input)\n",
    "        x = self.act(x)  # GELU\n",
    "        x = self.dropout(x)\n",
    "        logits = self.out_proj(x)  # [Batch_Size x 5]\n",
    "\n",
    "        loss = None\n",
    "        \n",
    "        if labels != None:\n",
    "            loss_fct = nn.L1Loss()\n",
    "\n",
    "           # loss_fct = loss_fct(logits_O.view(-1, self.num_labels), labels[0])\n",
    "            loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "\n",
    "        return loss, logits\n",
    "    \n",
    "electra_tokenizer = AutoTokenizer.from_pretrained(KOELECTRA)\n",
    "bigbird_tokenizer = AutoTokenizer.from_pretrained(KOBIGBIRD)\n",
    "klue_tokenizer = AutoTokenizer.from_pretrained(KOROBERTA)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(KOBERT, trust_remote_code=True)\n",
    "\n",
    "test_ensemble = EnsembleModel(num_labels = 5)\n",
    "\n",
    "state_dict = torch.load(\"/mnt/HDD8TB/PersonalityAI/FI_1head/keti_koEnsemble_model_usingrobertacheck+kobert_concat_nosafetensor/checkpoint-70/pytorch_model.bin\")\n",
    "\n",
    "test_ensemble.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "def run(text):\n",
    "    jf_text1 = text\n",
    "    \n",
    "    tokens = CLIPDataset(data = jf_text1,\n",
    "                        electra_tokenizer = electra_tokenizer,\n",
    "                        bigbird_tokenizer = bigbird_tokenizer,\n",
    "                        klue_tokenizer = klue_tokenizer,\n",
    "                        bert_tokenizer = bert_tokenizer\n",
    "                       )\n",
    "    tokens = tokens.__getitem__(0)\n",
    "    print(tokens)\n",
    "    print(electra_tokenizer.decode(tokens['input_ids'][0]))\n",
    "    \n",
    "    test_ensemble.eval()\n",
    "    \n",
    "    _, result = test_ensemble(\n",
    "            input_ids = tokens['input_ids'],\n",
    "            attention_mask = tokens['attention_mask'],\n",
    "            token_type_ids = tokens['token_type_ids'],\n",
    "            input_ids_bird = tokens['input_ids_bird'],\n",
    "            attention_mask_bird = tokens['attention_mask_bird'],\n",
    "            token_type_ids_bird = tokens['token_type_ids_bird'],\n",
    "            input_ids_klue = tokens['input_ids_klue'],\n",
    "            attention_mask_klue = tokens['attention_mask_klue'],\n",
    "            token_type_ids_klue = tokens['token_type_ids_klue'],\n",
    "            input_ids_bert = tokens['input_ids_bert'],\n",
    "            attention_mask_bert = tokens['attention_mask_bert'],\n",
    "            token_type_ids_bert = tokens['token_type_ids_bert'],\n",
    "                )\n",
    "    \n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 - torch",
   "language": "python",
   "name": "python3-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "412.844px",
    "left": "1192.33px",
    "right": "20px",
    "top": "111px",
    "width": "314px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
