{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4870234c-1915-4797-953d-1443ab7bfb55",
   "metadata": {},
   "source": [
    "### Pre-defined Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "128e0133-7d02-407b-8dbb-0475283774bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def test_compute_metrics(label, pred):\n",
    "    MAE = []\n",
    "\n",
    "    for i in range(5):\n",
    "        if pred[i][-1] == '.':\n",
    "            pred[i] = pred[i][:-1]\n",
    "        \n",
    "        MAE.append(1 - mean_absolute_error([float(label[i])], [float(pred[i])]))\n",
    "    \n",
    "    return MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "478d07b6-2288-46a6-b9d5-45536894c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FI_test = pd.read_csv(\"/data/visi2/Dataset/KETI_val_from_ver0.4.csv\",\n",
    "                     index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41f717-db2d-492a-9bc5-91be3f15bc84",
   "metadata": {},
   "source": [
    "### Zero-shot LLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a93dd4-dd82-4968-8324-0f05fcc182a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, GenerationConfig\n",
    "import logging\n",
    "logging.disable(logging.INFO) \n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "FI_test = pd.read_csv(\"/data/visi2/Dataset/KETI_val_from_ver0.4.csv\")  # 데이터 불러오기 (First Impression V2 데이터의 Test set 불러오기)\n",
    "\n",
    "MODEL_DIR = \"/home/visi2/juphome/PersonalityAI/Instruction_tuning/Bllossom-8B/\"  # 모델 이름\n",
    "ORIGIN_MODEL_DIR = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"  # 모델 이름\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_DIR, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "print(model.num_parameters())  # 모델 사이즈 출력; KoSOLAR : 10,804,924,416 (10.8B)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ORIGIN_MODEL_DIR)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "zero_shot2 = '당신은 문장에 대해 OCEAN 성격 요소 각각에 대한 점수를 알려주는 전문가 입니다. \\n\\\n",
    "OCEAN 성격 요소는 다음에 대한 사항을 확인합니다. O(Openness), C(Conscientiousness), E(Extraversion), A(Agreeableness), N(Neuroticism) \\n\\\n",
    "주어진 문장을 읽고, 문장에서 나타나는 OCEAN 성격 요소들을 분석하여 각각의 요소에 0에서 1 사이의 점수를 매기세요. \\n\\\n",
    "점수는 문장에서 해당 요소가 얼마나 잘 드러나는지를 기반으로 하며, 0에 가까울수록 전혀 드러나지 않음을, 1에 가까울수록 매우 강하게 드러남을 의미합니다. \\n\\\n",
    "다음의 예시와 같이 출력 값으로 주어진 문장에 대한 OCEAN 성격 요소 5가지를 각각의 점수로 나타내어야합니다. [O: 0.6722, C: 0.8322, E: 0.4860, A: 0.5298, N: 0.9323] \\n\\\n",
    "'\n",
    "\n",
    "genconfig = GenerationConfig(  # Decoding/Sampling Strategy\n",
    "    max_new_tokens=50, # 최대 생성 토큰, 1024\n",
    "    do_sample=True, \n",
    "    min_new_tokens = 15,\n",
    "    temperature=0.9,\n",
    "    repetition_penalty=1.3,\n",
    "    top_k=50,\n",
    "    top_p=0.92,\n",
    "    eos_token_id=model.config.eos_token_id\n",
    ")\n",
    "\n",
    "result_df = pd.DataFrame(columns=[\"Text\", \"Inference\"])  # 결과를 저장할 데이터프레임\n",
    "\n",
    "for content in tqdm(FI_test['transcription'].to_list()):\n",
    "    user = content\n",
    "    inputs = tokenizer.encode(zero_shot2 + '문장: \"' + user + '\",\\n 결과- [O: ', return_tensors='pt').to(\"cuda\")  # 입력 데이터 토크나이징\n",
    "    outputs = model.generate(inputs, \n",
    "                             generation_config = genconfig)\n",
    "\n",
    "    # Post-Processing\n",
    "    out_text = tokenizer.decode([el.item() for el in outputs[0]])  # output 숫자 값 -> 텍스트로 바꾸기\n",
    "    out_text = out_text.replace(tokenizer.decode(inputs[0]), \"\")  # output에서 input 빼기\n",
    "\n",
    "    tmp_df = pd.DataFrame({'Text': [user], 'Inference': [out_text]})  # 만들어진 출력 저장하기\n",
    "    result_df = pd.concat([result_df, tmp_df])\n",
    "\n",
    "# KULLM, KoSOLAR (SOLAR-10.7B) : 약 21GB GPU 메모리 소모"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5a5538-e206-43ee-828c-bf42ef70b3c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df['Inference'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d7e3e6-317e-4883-83ce-d3283ec140bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "p = re.compile('0[.]\\d*')\n",
    "\n",
    "for content in result_df['Inference'].to_list():\n",
    "    tmp = p.findall(content)\n",
    "    score.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3560e6b1-794a-4a43-914c-fd9522332d22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b9f377-f144-44b1-837d-07d3b2d7f35c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FI_test = pd.read_csv(\"/data/visi2/Dataset/KETI_val_from_ver0.4.csv\",\n",
    "                     index_col=0)\n",
    "FI_test['label'] = FI_test['OCEAN'].map(lambda x : [float(i) for i in x[1:-1].split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802ac78-a140-4bb9-a037-6e1563c4a1dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAE_arr = []\n",
    "error_idx = []\n",
    "\n",
    "for i in range(len(FI_test)):\n",
    "    try:\n",
    "        MAE_arr.append(test_compute_metrics(FI_test['label'].iloc[i][:5], score[i][:5]))\n",
    "    except Exception as e:\n",
    "        print(i, e)\n",
    "        error_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a618ba-5703-422d-b5d2-7fb088b71c66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e862f-f785-4a93-b0b8-03d7ec81211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(error_idx) !=0:\n",
    "    re_df = pd.DataFrame(columns=[\"Text\", \"Inference\"])\n",
    "    \n",
    "    for data in tqdm(FI_test['transcription'][error_idx].to_list()):\n",
    "        user = data\n",
    "     #   conversation = [{'role': 'system', 'content': zero_shot2}, {'role' : 'user', 'content' : 'Given Sentence: \"' + user + '\",\\n Results- [O: '}]\n",
    "        inputs = tokenizer.encode(zero_shot2 + '문장: \"' + user + '\",\\n 결과- [O: ', return_tensors='pt').to(\"cuda\")  # No template\n",
    "        outputs = model.generate(inputs, \n",
    "                    #      streamer=streamer,\n",
    "                                 generation_config = genconfig)\n",
    "        \n",
    "        out_text = tokenizer.decode([el.item() for el in outputs[0]])\n",
    "        out_text = out_text.replace(tokenizer.decode(inputs[0]), \"\")\n",
    "    \n",
    "        tmp_df = pd.DataFrame({'Text': [user], 'Inference': [out_text]})\n",
    "        re_df = pd.concat([re_df, tmp_df])\n",
    "\n",
    "    re_score = []\n",
    "    p = re.compile('0[.]\\d*')\n",
    "    \n",
    "    for content in re_df['Inference'].to_list():\n",
    "        tmp = p.findall(content)\n",
    "        re_score.append(tmp)\n",
    "        \n",
    "    for i, idx in enumerate(error_idx):\n",
    "        score[idx] = re_score[i]\n",
    "\n",
    "    MAE_arr = []\n",
    "    error_idx = []\n",
    "    \n",
    "    for i in range(len(FI_test)):\n",
    "        try:\n",
    "            MAE_arr.append(test_compute_metrics(FI_test['label'].iloc[i][:5], score[i][:5]))\n",
    "        except Exception as e:\n",
    "            print(i, e)\n",
    "            error_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a338573-4e20-4544-bacd-b4a3b74e95ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_mae = np.array(MAE_arr).mean(axis=0)\n",
    "avg_mae = np.mean(factor_mae)\n",
    "\n",
    "factor_mae, avg_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c17dc8d-1bc1-4743-8ee2-118bbfcd4241",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678e6863-7a31-447f-aee6-ff6661d2f85c",
   "metadata": {},
   "source": [
    "### One-shot LLM Inference (Kernel Restart Required for GPU Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e382480c-9dff-45e7-83d5-cc96ae477ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, GenerationConfig\n",
    "import logging\n",
    "import pandas as pd\n",
    "logging.disable(logging.INFO) # disable INFO and DEBUG logging everywhere\n",
    "logging.disable(logging.WARNING) # disable WARNING, INFO and DEBUG logging everywhere\n",
    "\n",
    "FI_test = pd.read_csv(\"/data/visi2/Dataset/KETI_val_from_ver0.4.csv\")\n",
    "\n",
    "MODEL_DIR = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "ORIGIN_MODEL_DIR = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"  # 모델 이름\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_DIR, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "print(model.config.eos_token_id)\n",
    "print(model.num_parameters())  # KoSOLAR : 10,804,924,416 (10.8B)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ORIGIN_MODEL_DIR)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "one_shot_text = \"현재 가장 좋아하는 책 사실 몇 달 전에 서점에서 이 책을 샀는데 읽을 기회가 없었는데 읽기 시작했어요 한번 시작하면 내려놓고 싶지 않은 책 조이스 마이어의 <용감하게 살기>라는 책이에요.\"\n",
    "\n",
    "zero_shot2 = '당신은 문장에 대해 OCEAN 성격 요소 각각에 대한 점수를 알려주는 전문가 입니다. \\n\\\n",
    "OCEAN 성격 요소는 다음에 대한 사항을 확인합니다. O(Openness), C(Conscientiousness), E(Extraversion), A(Agreeableness), N(Neuroticism) \\n\\\n",
    "주어진 문장을 읽고, 문장에서 나타나는 OCEAN 성격 요소들을 분석하여 각각의 요소에 0에서 1 사이의 점수를 매기세요. \\n\\\n",
    "점수는 문장에서 해당 요소가 얼마나 잘 드러나는지를 기반으로 하며, 0에 가까울수록 전혀 드러나지 않음을, 1에 가까울수록 매우 강하게 드러남을 의미합니다. \\n\\\n",
    "다음의 예시와 같이 출력 값으로 주어진 문장에 대한 OCEAN 성격 요소 5가지를 각각의 점수로 나타내어야합니다. [O: 0.6722, C: 0.8322, E: 0.4860, A: 0.5298, N: 0.9323] \\n\\\n",
    "'\n",
    "\n",
    "ko_prompt_one_shot = zero_shot2 + '문장: \"' + one_shot_text + '\",\\n 결과- [O: 0.6333, C: 0.7184, E: 0.4019, A: 0.4176, N: 0.5313] \\n'\n",
    "\n",
    "genconfig = GenerationConfig(\n",
    "    max_new_tokens=50, \n",
    "    do_sample=True, \n",
    "    min_new_tokens = 15,\n",
    "    temperature=0.9,\n",
    "    repetition_penalty=1.3,\n",
    " #   no_repeat_ngram_size=3,\n",
    "    top_k=50,\n",
    "    top_p=0.92,\n",
    "    eos_token_id=model.config.eos_token_id\n",
    ")\n",
    "\n",
    "result_df = pd.DataFrame(columns=[\"Text\", \"Inference\"])\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for content in tqdm(FI_test['transcription'].to_list()):\n",
    "    user = content\n",
    "    inputs = tokenizer.encode(ko_prompt_one_shot + '문장: \"' + user + '\",\\n 결과- [O: ', return_tensors='pt').to(\"cuda\")  \n",
    "\n",
    "\n",
    "    outputs = model.generate(inputs, \n",
    "                             generation_config = genconfig)\n",
    "    out_text = tokenizer.decode([el.item() for el in outputs[0]])\n",
    "    out_text = out_text.replace(tokenizer.decode(inputs[0]), \"\")\n",
    "\n",
    "    tmp_df = pd.DataFrame({'Text': [user], 'Inference': [out_text]})\n",
    "    result_df = pd.concat([result_df, tmp_df])\n",
    "\n",
    "# KULLM, KoSOLAR (SOLAR-10.7B) : 약 21GB GPU 메모리 소모"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc460f9b-cdf3-4c86-a7f6-cf43eedad4c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df['Inference'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3528e4cf-5d5c-409c-bd1d-824d07f15e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "p = re.compile('0[.]\\d*')\n",
    "\n",
    "for content in result_df['Inference'].to_list():\n",
    "    tmp = p.findall(content)\n",
    "    score.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bd5e9e-68d2-451f-bfe6-21dc57bb11da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c51395-6d24-4a6d-9ad7-3cc7421eee46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FI_test = pd.read_csv(\"/data/visi2/Dataset/KETI_val_from_ver0.4.csv\",\n",
    "                     index_col=0)\n",
    "FI_test['label'] = FI_test['OCEAN'].map(lambda x : [float(i) for i in x[1:-1].split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4400be4e-6b26-4bb7-8c34-f0ebbf13d3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAE_arr = []\n",
    "error_idx = []\n",
    "\n",
    "for i in range(len(FI_test)):\n",
    "    try:\n",
    "        MAE_arr.append(test_compute_metrics(FI_test['label'].iloc[i][:5], score[i][:5]))\n",
    "    except Exception as e:\n",
    "        print(i, e)\n",
    "        error_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dbabba-6479-4b87-9921-de0705d88b50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf0dfc6-24a7-4181-9e92-98c2a47830ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(error_idx) !=0:\n",
    "    re_df = pd.DataFrame(columns=[\"Text\", \"Inference\"])\n",
    "    \n",
    "    for data in tqdm(FI_test['transcription'][error_idx].to_list()):\n",
    "        user = data\n",
    "       # conversation = [{'role': 'system', 'content': en_prompt}, {'role' : 'user', 'content' : 'Given Sentence: \"' + user + '\",\\n Results- [O: '}]\n",
    "        inputs = tokenizer.encode(ko_prompt_one_shot + '문장: \"' + user + '\",\\n 결과- [O: ', return_tensors='pt').to(\"cuda\")  # No template\n",
    "        outputs = model.generate(inputs, \n",
    "                    #      streamer=streamer,\n",
    "                                 generation_config = genconfig)\n",
    "        \n",
    "        out_text = tokenizer.decode([el.item() for el in outputs[0]])\n",
    "        out_text = out_text.replace(tokenizer.decode(inputs[0]), \"\")\n",
    "    \n",
    "        tmp_df = pd.DataFrame({'Text': [user], 'Inference': [out_text]})\n",
    "        re_df = pd.concat([re_df, tmp_df])\n",
    "\n",
    "    re_score = []\n",
    "    p = re.compile('0[.]\\d*')\n",
    "    \n",
    "    for content in re_df['Inference'].to_list():\n",
    "        tmp = p.findall(content)\n",
    "        re_score.append(tmp)\n",
    "        \n",
    "    for i, idx in enumerate(error_idx):\n",
    "        score[idx] = re_score[i]\n",
    "\n",
    "    MAE_arr = []\n",
    "    error_idx = []\n",
    "    \n",
    "    for i in range(len(FI_test)):\n",
    "        try:\n",
    "            MAE_arr.append(test_compute_metrics(FI_test['label'].iloc[i][:5], score[i][:5]))\n",
    "        except Exception as e:\n",
    "            print(i, e)\n",
    "            error_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8392b9d-faea-4ea8-a7a5-e7e3c3046f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_mae = np.array(MAE_arr).mean(axis=0)\n",
    "avg_mae = np.mean(factor_mae)\n",
    "\n",
    "factor_mae, avg_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d6efad-e500-4f34-9de1-1ca5f384d976",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078511cd-4367-4681-8e08-94e036536182",
   "metadata": {},
   "source": [
    "### five-shot LLM Inference (Kernel Restart Required for GPU Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf3e88f-3af2-4409-a8c1-63e309e612d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, GenerationConfig\n",
    "import logging\n",
    "import pandas as pd\n",
    "logging.disable(logging.INFO) # disable INFO and DEBUG logging everywhere\n",
    "logging.disable(logging.WARNING) # disable WARNING, INFO and DEBUG logging everywhere\n",
    "\n",
    "FI_test = pd.read_csv(\"/data/visi2/Dataset/KETI_val_from_ver0.4.csv\")\n",
    "\n",
    "MODEL_DIR = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "ORIGIN_MODEL_DIR = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"  # 모델 이름\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_DIR, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "print(model.config.eos_token_id)\n",
    "print(model.num_parameters())  # KoSOLAR : 10,804,924,416 (10.8B)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ORIGIN_MODEL_DIR)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "few_shot_text0 = \"네 그럼 14위는 어떤 분으로 하는 게 좋을까요?\"\n",
    "few_shot_text1 = \"편의점 귀엽다\"\n",
    "few_shot_text2 = \"술 팔까요? 아 근데\"\n",
    "few_shot_text3 = \"진짜요? 그러면 해주실 수 있어요.\"\n",
    "few_shot_text4 = \"그렇죠 그렇죠 그렇죠 여기 여기 메인 무대가 무대랑 좌석인 거죠?\"\n",
    "\n",
    "zero_shot2 = '당신은 문장에 대해 OCEAN 성격 요소 각각에 대한 점수를 알려주는 전문가 입니다. \\n\\\n",
    "OCEAN 성격 요소는 다음에 대한 사항을 확인합니다. O(Openness), C(Conscientiousness), E(Extraversion), A(Agreeableness), N(Neuroticism) \\n\\\n",
    "주어진 문장을 읽고, 문장에서 나타나는 OCEAN 성격 요소들을 분석하여 각각의 요소에 0에서 1 사이의 점수를 매기세요. \\n\\\n",
    "점수는 문장에서 해당 요소가 얼마나 잘 드러나는지를 기반으로 하며, 0에 가까울수록 전혀 드러나지 않음을, 1에 가까울수록 매우 강하게 드러남을 의미합니다. \\n\\\n",
    "다음의 예시와 같이 출력 값으로 주어진 문장에 대한 OCEAN 성격 요소 5가지를 각각의 점수로 나타내어야합니다. [O: 0.6722, C: 0.8322, E: 0.4860, A: 0.5298, N: 0.9323] \\n\\\n",
    "'\n",
    "\n",
    "ko_prompt_few_shot = zero_shot2 + '문장: \"' + few_shot_text0 + '\",\\n 결과- [O: 0.9762 , C: 0.9792 , E: 0.8333 , A: 0.6458 , N: 0.2083 ] \\n' + \\\n",
    "                    '문장: \"' + few_shot_text1 + '\",\\n 결과- [O: 0.6667 , C: 0.7292, E: 0.8542, A: 0.5625, N: 0.2708] \\n' + \\\n",
    "                    '문장: \"' + few_shot_text2 + '\",\\n 결과- [O: 0.5625, C: 0.8333, E: 0.8125, A: 0.3125, N: 0.2708 ] \\n' + \\\n",
    "                    '문장: \"' + few_shot_text3 + '\",\\n 결과- [O: 0.7917, C: 0.5625, E: 0.8542, A: 0.7083, N: 0.3750] \\n' + \\\n",
    "                    '문장: \"' + few_shot_text4 + '\",\\n 결과- [O: 0.9375, C: 0.9583, E: 0.8750, A: 0.3542, N: 0.1875 ] \\n'\n",
    "\n",
    "\n",
    "genconfig = GenerationConfig(\n",
    "    max_new_tokens=50, \n",
    "    do_sample=True, \n",
    "    min_new_tokens = 15,\n",
    "    temperature=0.9,\n",
    "    repetition_penalty=1.3,\n",
    " #   no_repeat_ngram_size=3,\n",
    "    top_k=50,\n",
    "    top_p=0.92,\n",
    "    eos_token_id=model.config.eos_token_id\n",
    ")\n",
    "\n",
    "result_df = pd.DataFrame(columns=[\"Text\", \"Inference\"])\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for content in tqdm(FI_test['transcription'].to_list()):\n",
    "    user = content\n",
    "    inputs = tokenizer.encode(ko_prompt_few_shot + '문장: \"' + user + '\",\\n 결과- [O: ', return_tensors='pt').to(\"cuda\") \n",
    "\n",
    "    outputs = model.generate(inputs, \n",
    "                #      streamer=streamer,\n",
    "                             generation_config = genconfig)\n",
    "    \n",
    "    out_text = tokenizer.decode([el.item() for el in outputs[0]])\n",
    "    out_text = out_text.replace(tokenizer.decode(inputs[0]), \"\")\n",
    "\n",
    "    tmp_df = pd.DataFrame({'Text': [user], 'Inference': [out_text]})\n",
    "    result_df = pd.concat([result_df, tmp_df])\n",
    "\n",
    "# KULLM, KoSOLAR (SOLAR-10.7B) : 약 21GB GPU 메모리 소모"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d3d616-c3ab-4c5e-99d9-3c33548c5900",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df['Inference'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19ebdb9-c0fd-4fed-ba0e-027d28eb9583",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "p = re.compile('0[.]\\d*')\n",
    "\n",
    "for content in result_df['Inference'].to_list():\n",
    "    tmp = p.findall(content)\n",
    "    score.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7547eddb-5874-47d8-97a9-df236dd8a853",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a0825-a757-4f93-82a1-209ac0225df7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FI_test = pd.read_csv(\"/data/visi2/Dataset/KETI_val_from_ver0.4.csv\",\n",
    "                     index_col=0)\n",
    "FI_test['label'] = FI_test['OCEAN'].map(lambda x : [float(i) for i in x[1:-1].split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3064c3f3-3905-444f-8c9c-bd243cbacba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAE_arr = []\n",
    "error_idx = []\n",
    "\n",
    "for i in range(len(FI_test)):\n",
    "    try:\n",
    "        MAE_arr.append(test_compute_metrics(FI_test['label'].iloc[i][:5], score[i][:5]))\n",
    "    except Exception as e:\n",
    "        print(i, e)\n",
    "        error_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37db3421-eb63-4704-98e3-02e038390ef5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cb0b92-d039-4129-a36c-ba6760944a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(error_idx) !=0:\n",
    "    re_df = pd.DataFrame(columns=[\"Text\", \"Inference\"])\n",
    "    \n",
    "    for data in tqdm(FI_test['transcription'][error_idx].to_list()):\n",
    "        user = data\n",
    "     #   conversation = [{'role': 'system', 'content': en_prompt}, {'role' : 'user', 'content' : 'Given Sentence: \"' + user + '\",\\n Results- [O: '}]\n",
    "        inputs = tokenizer.encode(ko_prompt_few_shot + '문장: \"' + user + '\",\\n 결과- [O: ', return_tensors='pt').to(\"cuda\")  # No template\n",
    "        outputs = model.generate(inputs, \n",
    "                    #      streamer=streamer,\n",
    "                                 generation_config = genconfig)\n",
    "        \n",
    "        out_text = tokenizer.decode([el.item() for el in outputs[0]])\n",
    "        out_text = out_text.replace(tokenizer.decode(inputs[0]), \"\")\n",
    "    \n",
    "        tmp_df = pd.DataFrame({'Text': [user], 'Inference': [out_text]})\n",
    "        re_df = pd.concat([re_df, tmp_df])\n",
    "\n",
    "    re_score = []\n",
    "    p = re.compile('0[.]\\d*')\n",
    "    \n",
    "    for content in re_df['Inference'].to_list():\n",
    "        tmp = p.findall(content)\n",
    "        re_score.append(tmp)\n",
    "        \n",
    "    for i, idx in enumerate(error_idx):\n",
    "        score[idx] = re_score[i]\n",
    "\n",
    "    MAE_arr = []\n",
    "    error_idx = []\n",
    "    \n",
    "    for i in range(len(FI_test)):\n",
    "        try:\n",
    "            MAE_arr.append(test_compute_metrics(FI_test['label'].iloc[i][:5], score[i][:5]))\n",
    "        except Exception as e:\n",
    "            print(i, e)\n",
    "            error_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a16725e-bc1f-402c-9f95-703e09f3a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_mae = np.array(MAE_arr).mean(axis=0)\n",
    "avg_mae = np.mean(factor_mae)\n",
    "\n",
    "factor_mae, avg_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114e8690-74ab-43d5-a770-494701ed3a08",
   "metadata": {},
   "source": [
    "### ETC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa08a594-99b1-40e8-95fd-be6d9ad80c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = \"너는 지금부터 문장에 대해 BIG 5 성격 검사의 결과를 보여주는 전문 계산기야. 너는 전문가로써 BIG 5에 대한 정보를 충분히 알고있고 \\\n",
    "주어진 문장에 대해 5가지 요소를 가지는 BIG 5 성격 검사로 개성을 평가해야만 해. \\\n",
    "이 때, 문장이 주어지면 각각의 요소(개방성, 성실성, 우호성, 외향성, 신경증)에 대해 0부터 1사이의 값을 가지도록 숫자로 평가해야하고, \\\n",
    "모든 요소에 대해 최대한 세밀하고 정확하게 값을 출력해야 해. \\\n",
    "결과를 출력할 때, BIG 5에 대한 설명이나 이유에 대해서는 필요 없이 숫자 값만 보여줘. <요소 - 숫자> 형식으로 출력해.\\\n",
    "문장 : 오늘 워크샵은 정말 기대돼!>\"\n",
    "\n",
    "p2 = \"당신은 문장에 대해 BIG 5 성격 검사의 결과를 보여주는 전문가 입니다. 주어진 문장을 읽고, 다음과 같은 OCEAN 요소들을 각각 0에서 1 사이의 값으로 나타내세요. O(Openness): 개방성, C(Conscientiousness): 성실성, \\\n",
    "E(Extraversion): 외향성, A(Agreeableness): 친화성, N(Neuroticism): 신경성, 문장을 이용하여 각 요소의 점수를 계산할 때, 다음과 같은 접근을 사용할 수 있습니다: \\\n",
    "Openness (O): 문장에 포함된 창의성, 호기심, 복잡성에 대한 언급 등을 고려합니다. Conscientiousness (C): 문장에서 나타나는 계획성, 철저함, 조직성을 고려합니다. \\\n",
    "Extraversion (E): 문장에서 외향적인 특성, 사회적 활동에 대한 언급을 고려합니다. Agreeableness (A): 문장에서 타인에 대한 관심, 협동성, 친화력을 고려합니다. \\\n",
    "Neuroticism (N): 문장에 포함된 감정적 불안, 긴장, 우울 등을 고려합니다. \\\n",
    "이러한 기준을 이용하여 주어진 문장을 분석하여 각 요소에 대한 0에서 1 사이의 값을 계산해보세요. \\\n",
    "계산된 값은 다음의 예시처럼 다섯 개의 요소에 대한 배열로 표현되어야 합니다. \\\n",
    "예시) 다른 방에서 미친 듯이 달려가서 없는지 확인했는데 없는 것 같았어요 이제 잠을 충분히 못 자서 가위를 어딘가로 옮겼을 수도 있지만 맹세컨대 - [0.8222, 0.6699, 0.4860, 0.6458, 0.6813]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a494656-4ede-4147-9a46-a503ccb96584",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = '당신은 문장에 대해 OCEAN 성격 요소 각각에 대한 점수를 알려주는 전문가 입니다. \\n\\\n",
    "OCEAN 성격 요소는 다음에 대한 사항을 확인합니다. O(Openness), C(Conscientiousness), E(Extraversion), A(Agreeableness), N(Neuroticism) \\n\\\n",
    "주어진 문장을 읽고, 문장에서 나타나는 OCEAN 성격 요소들을 분석하여 각각의 요소에 0에서 1 사이의 점수를 매기세요. \\n\\\n",
    "점수는 문장에서 해당 요소가 얼마나 잘 드러나는지를 기반으로 하며, 0에 가까울수록 전혀 드러나지 않음을, 1에 가까울수록 매우 강하게 드러남을 의미합니다. \\n\\\n",
    "다음의 예시와 같이 출력 값으로 주어진 문장에 대한 OCEAN 성격 요소 5가지를 각각의 점수로 나타내어야합니다. [O: 0.6722, C: 0.8322, E: 0.4860, A: 0.5298, N: 0.9323] \\n\\\n",
    "'\n",
    "one_shot_text = \"현재 가장 좋아하는 책 사실 몇 달 전에 서점에서 이 책을 샀는데 읽을 기회가 없었는데 읽기 시작했어요 한번 시작하면 내려놓고 싶지 않은 책 조이스 마이어의 <용감하게 살기>라는 책이에요.\"\n",
    "\n",
    "zero = base_prompt + '문장: \"' + \"<EXAMPLE SENTENCE OF FI_TEST DATASET>\" + '\",\\n 결과- [O: '\n",
    "\n",
    "one = base_prompt + '문장: \"' + one_shot_text + '\",\\n 결과- [O: 0.6333, C: 0.7184, E: 0.4019, A: 0.4176, N: 0.5313] \\n' + '문장: \"' + \"<EXAMPLE SENTENCE OF FI_TEST DATASET>\" + '\",\\n 결과- [O: '\n",
    "\n",
    "print(\"Zero shot Input: \", \"\\n\", zero )\n",
    "print(\"\\n\\n\" ,\"=========================\"*15, \"\\n\\n\")\n",
    "print(\"One shot Input: \", \"\\n\", one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4042fee-d714-425d-a470-96f4679f7d9c",
   "metadata": {},
   "source": [
    "### prompt example mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dcb70d-8c8a-4871-a4ae-51d144de5289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "val = pd.read_csv(\"/data/visi2/FI/KETI_test_from_ver0.4.csv\",\n",
    "                 index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65399b71-5d71-4cca-bfe2-10202cb9d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "val.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800ff3d7-40a4-4e15-9268-d836dedee2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_high = val[val['openness'] == val['openness'].max()]\n",
    "open_low = val.sort_values(by='openness')\n",
    "cons_high = val[val['conscientiousness'] == val['conscientiousness'].max()]\n",
    "cons_low = val[val['conscientiousness'] == val['conscientiousness'].min()]\n",
    "extra_high = val[val['extraversion'] == val['extraversion'].max()]\n",
    "extra_low = val.sort_values(by='extraversion')\n",
    "agree_high = val.sort_values(by='agreeableness',ascending=False)\n",
    "agree_low = val.sort_values(by='agreeableness')\n",
    "neuro_high = val[val['neuroticism'] == val['neuroticism'].max()]\n",
    "neuro_low = val.sort_values(by='neuroticism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3abb4b-a464-41ec-b597-1290c1cc758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuro_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a354e2-015d-4b14-8ce3-cd02f14d0aaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = {\n",
    "    open_high['transcription'].to_list()[0] : open_high[[\"openness\", \"conscientiousness\", \"extraversion\", \"agreeableness\", \"neuroticism\"]].iloc[0],\n",
    "    open_low['transcription'].to_list()[1] : open_low[[\"openness\", \"conscientiousness\", \"extraversion\", \"agreeableness\", \"neuroticism\"]].iloc[0],\n",
    "    cons_high['transcription'].to_list()[0] : cons_high[[\"openness\", \"conscientiousness\", \"extraversion\", \"agreeableness\", \"neuroticism\"]].iloc[0],\n",
    "    cons_low['transcription'].to_list()[0] : cons_low[[\"openness\", \"conscientiousness\", \"extraversion\", \"agreeableness\", \"neuroticism\"]].iloc[0],\n",
    "    extra_high['transcription'].to_list()[0] : extra_high[[\"openness\", \"conscientiousness\", \"extraversion\", \"agreeableness\", \"neuroticism\"]].iloc[0],\n",
    "    extra_low['transcription'].to_list()[1] : extra_low[[\"openness\", \"conscientiousness\", \"extraversion\", \"agreeableness\", \"neuroticism\"]].iloc[0],\n",
    "    agree_high['transcription'].to_list()[1] : agree_high[[\"openness\", \"conscientiousness\", \"extraversion\", \"agreeableness\", \"neuroticism\"]].iloc[0],\n",
    "    agree_low['transcription'].to_list()[1] : agree_low[[\"openness\", \"conscientiousness\", \"extraversion\", \"agreeableness\", \"neuroticism\"]].iloc[0],\n",
    "    neuro_high['transcription'].to_list()[0] : neuro_high[[\"openness\", \"conscientiousness\", \"extraversion\", \"agreeableness\", \"neuroticism\"]].iloc[0],\n",
    "    neuro_low['transcription'].to_list()[1] : neuro_low[[\"openness\", \"conscientiousness\", \"extraversion\", \"agreeableness\", \"neuroticism\"]].iloc[0],\n",
    "}\n",
    "\n",
    "len(res), res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f943c9cd-ab2e-4eff-b815-60286f95ee21",
   "metadata": {},
   "source": [
    "### Ten-Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937c4a93-5df4-43e3-b35b-268d6629982c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, GenerationConfig\n",
    "import logging\n",
    "import pandas as pd\n",
    "logging.disable(logging.INFO) # disable INFO and DEBUG logging everywhere\n",
    "logging.disable(logging.WARNING) # disable WARNING, INFO and DEBUG logging everywhere\n",
    "\n",
    "FI_test = pd.read_csv(\"/data/visi2/Dataset/KETI_val_from_ver0.4.csv\")\n",
    "\n",
    "MODEL_DIR = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "ORIGIN_MODEL_DIR = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"  # 모델 이름\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_DIR, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "print(model.config.eos_token_id)\n",
    "print(model.num_parameters())  # KoSOLAR : 10,804,924,416 (10.8B)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ORIGIN_MODEL_DIR)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "few_shot_text0 = \"고민 중인데 노라조, \\n\"\n",
    "few_shot_text1 = \"네 괜찮은 것 같아요. 그러면 4위 \\n\"\n",
    "few_shot_text2 = \"10분에 70만 원이요 거의 뭐 \\n\"\n",
    "few_shot_text3 = \"맞아 잔치국수 어 잔치국수 괜찮다 \\n\"\n",
    "few_shot_text4 = \"되게 빡센데 1박 2일이면 많은 걸 보여줘야 되는데 그것도 괜찮은데 \\n\"\n",
    "few_shot_text5 = \"쇼핑도 해야돼 저도 P에요 \\n\"\n",
    "few_shot_text6 = \"할 수 있는데 캐리커처는 약간 또 다른 재능의 영역이라고 생각해요. 그것도 약간 교육이 필요한 것 같은데 \\n\"\n",
    "few_shot_text7 = \"저는 우선 그래프를 봤을 때 연령대가 나오는데 일단 중년층이랑 고등학생과 이제 대학생 비율이 높다는 점을 고려를 해서 \\\n",
    "동서남북 지도를 살펴봤을 때 저는 남쪽이랑 동쪽이 좀 유리하다라고 생각이 들었고 그중에서도 동쪽이 가장 유리하지 않나라는 생각이 들었어요 \\\n",
    "우선 동쪽에는 나이 드신 분들이라 진학 상담 부스가 있어서 고등학생들이 많이 몰리게 되는데 딱 방문자 수 비율을 봤을 때 \\\n",
    "이제 중년 46세부터 55세 분들과 이제 16세부터 25세의 비율이 높다 보니까 좀 동쪽을 하면은 좀 유동인구 연령층과도 잘 맞아가지고 좋지 않을까라는 생각이 들었어요. \\n\"\n",
    "few_shot_text8 = \"강아지 키우세요? 귀여워요 어떡해 \\n\"\n",
    "few_shot_text9 = \"그래서 또 그럼 둘째 날 언제 가지? 점심에 가요? \\n\"\n",
    "\n",
    "zero_shot2 = '당신은 문장에 대해 OCEAN 성격 요소 각각에 대한 점수를 알려주는 전문가 입니다. \\n\\\n",
    "OCEAN 성격 요소는 다음에 대한 사항을 확인합니다. O(Openness), C(Conscientiousness), E(Extraversion), A(Agreeableness), N(Neuroticism) \\n\\\n",
    "주어진 문장을 읽고, 문장에서 나타나는 OCEAN 성격 요소들을 분석하여 각각의 요소에 0에서 1 사이의 점수를 매기세요. \\n\\\n",
    "점수는 문장에서 해당 요소가 얼마나 잘 드러나는지를 기반으로 하며, 0에 가까울수록 전혀 드러나지 않음을, 1에 가까울수록 매우 강하게 드러남을 의미합니다. \\n\\\n",
    "다음의 예시와 같이 출력 값으로 주어진 문장에 대한 OCEAN 성격 요소 5가지를 각각의 점수로 나타내어야합니다. [O: 0.6722, C: 0.8322, E: 0.4860, A: 0.5298, N: 0.9323] \\n\\\n",
    "'\n",
    "\n",
    "ko_prompt_few_shot = zero_shot2 + '문장: \"' + few_shot_text0 + '\",\\n 결과- [O: 0.6875, C: 0.4792, E: 0.4375, A: 0.7708, N: 0.4375] \\n' + \\\n",
    "                    '문장: \"' + few_shot_text1 + '\",\\n 결과- [O: 0.6667, C: 0.7292, E: 0.8542, A: 0.5625, N: 0.2708 ] \\n' + \\\n",
    "                    '문장: \"' + few_shot_text2 + '\",\\n 결과- [O: 0.5208, C: 0.6875, E: 0.7920, A: 0.6458, N: 0.2500 ] \\n' + \\\n",
    "                    '문장: \"' + few_shot_text3 + '\",\\n 결과- [O: 0.4583, C: 0.6875, E: 0.7083, A: 0.5833, N: 0.3333] \\n' + \\\n",
    "                    '문장: \"' + few_shot_text4 + '\",\\n 결과- [O: 0.5000, C: 0.5625, E: 0.6875, A: 0.5625, N: 0.2708 ] \\n' + \\\n",
    "                    '문장: \"' + few_shot_text5 + '\",\\n 결과- [O: 0.5833, C: 0.4583, E: 0.8125, A: 0.5420, N: 0.4583] \\n' + \\\n",
    "                    '문장: \"' + few_shot_text6 + '\",\\n 결과- [O: 0.7292, C: 0.4792, E: 0.5000, A: 0.4583, N: 0.7292 ] \\n' + \\\n",
    "                    '문장: \"' + few_shot_text7 + '\",\\n 결과- [O: 0.9792, C: 0.9792, E: 0.8333, A: 0.6458, N: 0.2083] \\n' + \\\n",
    "                    '문장: \"' + few_shot_text8 + '\",\\n 결과- [O: 0.7920, C: 0.5625, E: 0.8542, A: 0.7083, N: 0.3750] \\n' + \\\n",
    "                    '문장: \"' + few_shot_text9 + '\",\\n 결과- [O: 0.5625, C: 0.8333, E: 0.8125, A: 0.3125, N: 0.2708] \\n'\n",
    "\n",
    "genconfig = GenerationConfig(\n",
    "    max_new_tokens=50, \n",
    "    do_sample=True, \n",
    "    min_new_tokens = 15,\n",
    "    temperature=0.9,\n",
    "    repetition_penalty=1.3,\n",
    "    top_k=50,\n",
    "    top_p=0.92,\n",
    "    eos_token_id=model.config.eos_token_id\n",
    ")\n",
    "\n",
    "result_df = pd.DataFrame(columns=[\"Text\", \"Inference\"])\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for content in tqdm(FI_test['transcription'].to_list()):\n",
    "    user = content\n",
    "    inputs = tokenizer.encode(ko_prompt_few_shot + '문장: \"' + user + '\",\\n 결과- [O: ', return_tensors='pt').to(\"cuda\") \n",
    "\n",
    "    outputs = model.generate(inputs, \n",
    "                             generation_config = genconfig)\n",
    "    \n",
    "    out_text = tokenizer.decode([el.item() for el in outputs[0]])\n",
    "    out_text = out_text.replace(tokenizer.decode(inputs[0]), \"\")\n",
    "\n",
    "    tmp_df = pd.DataFrame({'Text': [user], 'Inference': [out_text]})\n",
    "    result_df = pd.concat([result_df, tmp_df])\n",
    "\n",
    "# KULLM, KoSOLAR (SOLAR-10.7B) : 약 21GB GPU 메모리 소모"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21333f8-6e26-4810-b6a6-0ff13349dd77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df['Inference'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1228049-63b2-4c26-b5ee-824b8c49b4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "p = re.compile('0[.]\\d*')\n",
    "\n",
    "for content in result_df['Inference'].to_list():\n",
    "    tmp = p.findall(content)\n",
    "    score.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff3022-d8a4-47c5-85d7-02125f71cf62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc399d-7bfa-4ea5-b035-db28ec8431a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FI_test = pd.read_csv(\"/data/visi2/Dataset/KETI_val_from_ver0.4.csv\",\n",
    "                     index_col=0)\n",
    "FI_test['label'] = FI_test['OCEAN'].map(lambda x : [float(i) for i in x[1:-1].split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c059f61e-6292-4783-9a77-b30d3dc25caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_arr = []\n",
    "error_idx = []\n",
    "\n",
    "for i in range(len(FI_test)):\n",
    "    try:\n",
    "        MAE_arr.append(test_compute_metrics(FI_test['label'].iloc[i][:5], score[i][:5]))\n",
    "    except Exception as e:\n",
    "        print(i, e)\n",
    "        error_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b1857e-70a0-4c02-a5b6-7b17ae47d8e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c88856-9074-4886-92c5-9437c98a5be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while len(error_idx) !=0:\n",
    "    re_df = pd.DataFrame(columns=[\"Text\", \"Inference\"])\n",
    "    \n",
    "    for data in tqdm(FI_test['transcription'][error_idx].to_list()):\n",
    "        user = data\n",
    "     #   conversation = [{'role': 'system', 'content': en_prompt}, {'role' : 'user', 'content' : 'Given Sentence: \"' + user + '\",\\n Results- [O: '}]\n",
    "        inputs = tokenizer.encode(ko_prompt_few_shot + '문장: \"' + user + '\",\\n 결과- [O: ', return_tensors='pt').to(\"cuda\")  # No template\n",
    "        outputs = model.generate(inputs, \n",
    "                    #      streamer=streamer,\n",
    "                                 generation_config = genconfig)\n",
    "        \n",
    "        out_text = tokenizer.decode([el.item() for el in outputs[0]])\n",
    "        out_text = out_text.replace(tokenizer.decode(inputs[0]), \"\")\n",
    "    \n",
    "        tmp_df = pd.DataFrame({'Text': [user], 'Inference': [out_text]})\n",
    "        re_df = pd.concat([re_df, tmp_df])\n",
    "\n",
    "    re_score = []\n",
    "    p = re.compile('0[.]\\d*')\n",
    "    \n",
    "    for content in re_df['Inference'].to_list():\n",
    "        tmp = p.findall(content)\n",
    "        re_score.append(tmp)\n",
    "        \n",
    "    for i, idx in enumerate(error_idx):\n",
    "        score[idx] = re_score[i]\n",
    "\n",
    "    MAE_arr = []\n",
    "    error_idx = []\n",
    "    \n",
    "    for i in range(len(FI_test)):\n",
    "        try:\n",
    "            MAE_arr.append(test_compute_metrics(FI_test['label'].iloc[i][:5], score[i][:5]))\n",
    "        except Exception as e:\n",
    "            print(i, e)\n",
    "            error_idx.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da96e10e-992f-41a4-bc8a-b8d3ebd074ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_mae = np.array(MAE_arr).mean(axis=0)\n",
    "avg_mae = np.mean(factor_mae)\n",
    "\n",
    "factor_mae, avg_mae"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
